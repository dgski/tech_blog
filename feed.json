{
    "version": "https://jsonfeed.org/version/1",
    "title": "Tech @ DG",
    "description": "",
    "home_page_url": "https://tech.davidgorski.ca",
    "feed_url": "https://tech.davidgorski.ca/feed.json",
    "user_comment": "",
    "author": {
        "name": "David Gorski"
    },
    "items": [
        {
            "id": "https://tech.davidgorski.ca/what-nostr-the-decentralized-social-network-is-missing/",
            "url": "https://tech.davidgorski.ca/what-nostr-the-decentralized-social-network-is-missing/",
            "title": "Imagining Further Decentralization For NOSTR",
            "summary": "What is NOSTR? I’ve recently started looking at decentralized social network options.",
            "content_html": "<h1 id=\"what-is-nostr\">What is NOSTR?</h1>\n<p>I’ve recently started looking at decentralized social network options. They promise resistance to censorship and central control. One such network, or rather protocol, is NOSTR, which is an acronym for <strong>Notes and Other Stuff Transmitted by Relays</strong>. It’s adoption and potential is a function of its simplicity:</p>\n<ol>\n<li>Users generate a private/public key pair as an identity.</li>\n<li>Content is published via websocket to one or more relays. Signed with the private key.</li>\n<li>Content is requested via websocket from one or more relays. Verified with the public key.</li>\n</ol>\n<p>That’s pretty much it. While there are also some standards that define the JSON format for particular messages; the above list adequately summarizes how the entire network functions. When you ‘follow’ a user, in it’s simplest form, your NOSTR client will add them to a local list and request their content from the relays you are connected to. The users public key will be checked against each signature to ensure the relays aren’t ‘lying’ to you. By publishing and requesting content via multiple relays, one relay is not a single point of failure or control.</p>\n<h1 id=\"current-drawbacks\">Current Drawbacks</h1>\n<p>This is a good start and the simplicity is a key selling point. Simple technology can be adopted by developers and users much faster. I wrote a simple C++ NOSTR relay in a few hours myself: <a href=\"https://github.com/dgski/nostr_relay_lite\">nostr_relay_lite</a>. However, there are still some shortcomings when it comes to absolute full decentralization and central control resistance:</p>\n<ol>\n<li>Reliance on Relays.</li>\n<li>Reliance on the DNS (Domain Name System).</li>\n<li>Reliance on ISP (Internet Service Providers).</li>\n</ol>\n<h2 id=\"reliance-on-relays\">Reliance on Relays</h2>\n<p>The first obvious issue is that if all relays you publish to refuse to serve your content, none of your followers will be able to receive your content. Perhaps a powerful entity could enact pressure on the relays for them to do so. The solution, of course, is running your own relay. Unfortunately this introduces complexity and brittleness. You will have to set up and maintain a server while simultaneously broadcasting its existence. Other relays, already censoring your other content, would prevent the propagation of your endpoint.</p>\n<h2 id=\"reliance-on-dns\">Reliance on DNS</h2>\n<p>If you’ve begun hosting your own Relay, chances are you’ve put it behind a domain name. Unfortunately, the DNS system, controlled by a few select, organizations could easily ‘delist’ you. To prevent his you will have to use a raw static IP address instead of a domain name. This has its own obvious drawbacks.</p>\n<h2 id=\"reliance-on-isps\">Reliance on ISPs</h2>\n<p>So, you’ve stopped using DNS, and now you’re in the clear, right? Wrong. Your ISP could still cut you off. Just as with DNS or Relay providers, if enough pressure was enacted, your internet access could be pulled and your relay server would not be able to serve your content. This is the hardest layer of the network to ‘fix’ with current technology and infrastructure. I don’t have any actionable ideas here.</p>\n<h1 id=\"looking-forward-building-a-decentralized-holy-grail\">Looking Forward: Building A Decentralized Holy Grail</h1>\n<p>There is one big theme here: servers are the problem. For the purpose of decentralization, in an ideal world, users would communicate directly with each other; each client simultaneously functioning as a client and relay. </p>\n<p>Peer-to-Peer communication technologies have existed for quite some time and could be leveraged to address this, each user effectively hosting a server and a client simultaneously. There are networking tricks such as hole-punching to connect to devices directly behind NAT (Network Address Translation). Another technology to look at, as it has browser support, is WebRTC, although it currently requires an external channel for the initial connection. Perhaps it could be extended to allow one-way connections to open ports. IPv6 also might help here, as every machine could theoretically be granted a unique public IP address.</p>\n<p>Now regarding DNS, we can switch to a decentralized alternative. Rather than use the centralized DNS system managed by ICANN, clients could switch to using blockchain-based DNS. This would allow P2P relay owners to ‘publish’ the information needed to directly connect to their host machine. Even without a static IP address, a dynamic DNS feature could update this record as your IP address changes locally. An alternative to blockchain could be a DHT (Dynamic Hash Table) with private key signatures to ensure resolved IP address and port is approved by the user. This, however, would not allow for human-readable DNS and would simply be a mapping of public keys (identities) to IP addresses. Additionally, without a blockchain where there are codified rules for updating the ledger with payments, nodes could simply refuse to propagate your DHT entries, thus allowing for censorship.</p>\n<p>Finally, what about ISPs? No good answers here, unfortunately. The only solution would be build out a WLAN/Bluetooth Mesh Network that covers majority of the population. Devices would connect to one another directly via a wireless network and transmit packets (Prior art: FireChat, Amazon Sidewalk). This would be a huge challenge logistically and technologically; mesh networks are hard to scale. And just to add a touch of paranoia; some wired and line-of-sight links would also be necessary in the case of radio frequency jamming.</p>\n<p>Theoretically, one could build an integrated ‘NOSTR appliance’ that would allow connection to the mesh network, with a DDDNS (Decentralized Dynamic Domain Name System) or DHT (Decentralized Hash Table) and P2P networking built-in. This would allow non-tech users to link into this new ‘freedom network’; you just buy the product and get online; fully decentralized. Of course, this would not just be great for resisting censorship, but providing communication and connectivity in times of emergencies. To further remove centralization, this unit would ideally be powered via locally generated electricity to prevent your power provider from cutting off your ability to communicate.</p>\n<p>I want to finish off this post by saying that the value of decentralized systems is not only in their use, but in their existence. Having them standing by provides pressure on incumbent centralized systems not to push their luck when it comes to policy and enforcement. Perhaps decentralization to the level I’ve described here is not necessary, but somewhere between what we have now and my theoretical NOSTR appliance, is a decentralized system that is possible and practical. </p>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
            ],
            "date_published": "2024-03-20T13:11:49-04:00",
            "date_modified": "2024-03-20T13:11:49-04:00"
        },
        {
            "id": "https://tech.davidgorski.ca/introduction-to-low-latency-programming-clarify-program-scope/",
            "url": "https://tech.davidgorski.ca/introduction-to-low-latency-programming-clarify-program-scope/",
            "title": "Introduction To Low Latency Programming: Clarify Program Scope",
            "summary": "This post originally appears as a chapter in my new book: ‘Introduction&hellip;",
            "content_html": "<p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href=\"https://a.co/d/0U6KOfb\">Available now for purchase on Amazon</a>.</em></p>\n<p>I would like to begin this chapter by referencing a quote: “Measure twice, cut once.” While this statement is almost a platitude, it can be a useful reminder for careful, deliberate planning before taking action. I think taking this idea to heart is very important when it comes to low latency programming. The earlier in the development process you include low latency objectives, the better your software design will be and the better the eventual outcomes. Just as with good software engineering in general, we start by clarifying the program scope. Let’s ask ourselves two questions:</p>\n<ol>\n<li>What does the program <em>need</em> to do?</li>\n<li>What does <em>low latency</em> mean in the context of this program?</li>\n</ol>\n<p>Take a few minutes to ponder the two questions before I unpack their relevance in the following paragraphs. This is a good learning practice in general as it helps prepare your mind for ingesting new information.</p>\n<h2 id=\"functional-requirements\">Functional Requirements</h2>\n<p>Let’s think about the first question: What does the program <em>need</em> to do?</p>\n<p>Firstly, I want to go over a semantic detail. I use the word program above, but the question applies to any level of abstraction; from high-level to low-level, from distributed system to a single function. Apply the inquiry to whatever you are trying to build or optimize for low latency.</p>\n<p>Secondly, notice the emphasis on the word ‘need’. You are not asking yourself about wants or nice-to-haves. You are strictly thinking about what the functional unit cannot exist without. While this sounds like an obvious question, it actually helps us transition to a different mindset than latency-agnostic programming. ‘Everything’ that is not necessary should be potentially be removed. Let me list some examples to illuminate what I mean. Don’t use a data structure that maintains a sorted order if it is not necessary. Don’t log unless it is necessary. Don’t make a copy of data unless it is necessary. Don’t publish the data to an archive unless it is necessary. Don’t use a library that does more than you need. You <em>must</em> adopt a ruthless mindset when it comes to ripping out functionality and execution steps. Everything must justify its existence in your systems and code.</p>\n<p>Sometimes, asking this question ‘hard enough’ will reach back and influence your functional business requirements. If the operations necessary for enabling a feature are deemed ‘too slow’ in the best case, perhaps the business requirements are flawed and must be changed. For most domains, there is a two-way relationship between low-latency abilities and business requirements, as lower latency can potentially open up new business opportunities/functionality, and business requirements can sometimes help push for new latency standards that were once thought impossible. There is a delicate balance to be found and a healthy relationship between the two is important for overall success.</p>\n<h2 id=\"defining-low-latency-metrics\">Defining Low Latency Metrics</h2>\n<p>And now let’s shift our attention to the second question: What does <em>low latency</em> mean in the context of this program?</p>\n<p>Answering this question will help us define metrics and goals. When defining your metrics you must identify relevant operations and their respective start and end points. For example, if you are writing a web server this could be the ‘request to response’ duration. Or if you are optimizing a single function, it is the ‘call to return’ duration.</p>\n<p>As for defining goals, there are two different options. Either you define a strict, absolute time duration requirement for the relevant operations or simply give yourself a mandate to ‘be as fast as possible’. The former has a strict cut-off, duration wise, as to what is acceptable. The latter simply means you are aiming to reduce the duration between start to end time points as much as you can. While being ‘as fast as possible’ is always a good goal to have, the specific acceptable durations can be crucial from a business perspective and can help provide the push necessary to truly focus on lower latency.</p>\n<p>Preparing the metrics and their related goals are incredibly important.</p>\n<p>If the goal is a strict absolute time duration requirement it will give us an opportunity to analyze and determine whether it is even possible to fulfill; preventing wasted development time. For example: a business requirement requires <em>parsing and processing</em> a large incoming JSON message in 50 microseconds. You know that the absolute fastest parsing of a JSON message of that size on your target host/architecture takes 55 microseconds. Taking these facts into consideration, you will know it is simply an impossible ask.</p>\n<p>Additionally, it helps us to define measurable objectives for project success. This is important in any endeavor, programming or otherwise. If you don’t have a way to measure your deliverables, you simply won’t know if you are succeeded or failing. Having these measurable objectives will also be useful during development time to track your progress and inform you if your program design is working.</p>\n<h2 id=\"chapter-summary\">Chapter Summary</h2>\n<ul>\n<li>Clarify your functional requirements. Be extra aggressive when throwing out unnecessary features.</li>\n<li>Understand the two-way relationship between low latency objectives and business requirements.</li>\n<li>Define your low latency metrics. Use these throughout development and completion to measure project success.</li>\n</ul>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
                   "Programming",
                   "Low Latency",
                   "C++"
            ],
            "date_published": "2024-03-18T11:09:52-04:00",
            "date_modified": "2024-03-18T11:09:52-04:00"
        },
        {
            "id": "https://tech.davidgorski.ca/why-i-still-find-programming-inspiring-7-years-later/",
            "url": "https://tech.davidgorski.ca/why-i-still-find-programming-inspiring-7-years-later/",
            "title": "Why I Still Find Programming Inspiring, 8 Years Later",
            "summary": "I’ve been programming for 8 years now. While there have been ups&hellip;",
            "content_html": "<p>I’ve been programming for 8 years now. While there have been ups and downs, I’ve recently had a chance to reflect on my dual choice in career and hobby. It still means a lot to me.</p>\n<p>As an act of creative expression, it is truly unique. Programming is the act of creating <em>moving poetry</em>. Programs are not merely words at rest, they define and prescribe actions that will take place and interact with each other. As a form of writing it is also much more dynamic; as code is more likely to be updated than articles, books or blogs.</p>\n<p>It also includes a foil to the creativity. There is a requirement for the application of the muse to be logically consistency. Code isn’t just free-for-all modern art. You have real rules to play within; providing a grounded world to inhabit. This ‘grounding’ provides objectivity to the sport and allows comparison and competition between pieces.</p>\n<p>Also, programming is hilariously useful. It enables our modern world to run. To re-quote for the millionth time: <em>Software is eating the world.</em> And it is not for nothing: automation frees humanity of mundane, repeat tasks while unlocking a new form of leverage. Write once; run infinity times. This is especially mind-boggling when realizing the reduced capital requirements in the face of traditional industries.</p>\n<p>It can be easy to become jaded towards one career and one’s craft. The fun, art, hacking is often overshadowed by business requirements and politics. I get that. But, when you pause and think, it is truly magical we get to make money with intellectual self-reflection.</p>\n<p>I realized that this is precisely the dark side of it all; the self is a little too involved in the job. It touches the ego, the intellect and the creative identity. For such a well-compensated and flexible job, the number of depressed and burnt-out developers is quite high.</p>\n<p>It has been prognosticated many times that the end of programming is nigh; thanks to the development of new no-code tools. And maybe this time, with AI and large language models, that will be true. But that doesn’t change that for a brief moment in history, humans could talk to machines using their native language. And that’s more than kinda neat.</p>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
            ],
            "date_published": "2024-03-13T14:31:52-04:00",
            "date_modified": "2024-03-13T14:46:39-04:00"
        },
        {
            "id": "https://tech.davidgorski.ca/introduction-to-low-latency-programming-external-processing/",
            "url": "https://tech.davidgorski.ca/introduction-to-low-latency-programming-external-processing/",
            "title": "Introduction To Low Latency Programming: External Processing",
            "summary": "This post originally appears as a chapter in my new book: ‘Introduction&hellip;",
            "content_html": "<p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href=\"https://a.co/d/0U6KOfb\">Available now for purchase on Amazon</a>.</em></p>\n<p>After heavily scrutinizing your program scope you should be left with some functional requirements that are absolutely mandatory, as well as some latency goals you are aiming for. From here, you can start thinking about which pieces of the program could potentially be extracted from the main execution path, to keep it as fast as possible. For our purposes, the <em>main execution path or task</em> is process, system or function that is measured by our defined metrics. Here are some questions you can ask yourself to prepare for this process:</p>\n<ul>\n<li>What is known before the program even runs?</li>\n<li>Can we use this information to ‘do’ some steps before runtime?</li>\n<li>Does any part of the runtime process need to be done ‘real time’?</li>\n<li>Can any of the runtime steps be isolated into a task that can be packaged into a cohesive unit?</li>\n<li>What is the potential cost of communicating with the main task?</li>\n</ul>\n<h2 id=\"pre-processing\">Pre-processing</h2>\n<p>When we are discussing ‘low latency’ we are almost always referring to latency at runtime. Therefore, if we can remove segments of the program functionality and execute them before we enter our low latency execution; we can effectively get them done for ‘free’.</p>\n<p>If you have a lot of information that is used as an input to the runtime computation available to you before the process is even started, you can attempt to perform some parts of the computation even before the program is run. The following subsections discuss some common approaches.</p>\n<h3 id=\"configurationinput-files--loading-on-start-up\">Configuration/Input Files + Loading On Start Up</h3>\n<p>One approach is to generate configuration/input files and supply them to the process at startup. This is very simple from an operation and code perspective. You can create a separate process, or group of processes, that reads from any number of inputs, performs necessary computations and then outputs the results into a file. This file is then read by the low-latency process at start up. Steps:</p>\n<ol>\n<li>Run external process that completes and generates a file.</li>\n<li>Start the low-latency task/runtime, supplying the file as input.</li>\n</ol>\n<p>Additionally, if you don’t need the preparation functionality to be uncoupled from the process, you can have the main process itself do this computation before it enters the low-latency execution phase. This is simple and has an even lower operational overhead as the preparation code lives in the same code ‘space’ as the low-latency code. Warming process steps:</p>\n<ol>\n<li>Warming up: read various database tables into memory, calculate some weights, etc.</li>\n<li>Enter low-latency runtime loop.</li>\n</ol>\n<h3 id=\"compile-time-processing\">Compile Time Processing</h3>\n<p>Another option is ‘hard-coding’ computation into the executable with compile-time programming options: code generation, templates, and ‘constexpr’ functions, methods and variables. These approaches can help reduce instruction count/cost and branching; both key goals of low latency programming (discussed later in the book). While they can provide ‘the fastest’ runtime, they may also be difficult to use in comparison to runtime options. However, they do allow us to squeeze out maximum performance and are often worth pursuing. They are ‘external’ in the sense that they don’t occur during our low-latency runtime operations, because they don’t occur at all. The removal decision is in some sense a type of processing.</p>\n<h4 id=\"code-generation\">Code Generation</h4>\n<p>Code generation takes on many forms. It is the creation of programs that generate code as output. The inputs provided to the code generation program will determine the end functionality included in the final code. This allows us remove ‘work’ and decision-making out of the final runtime and perform these ‘steps’ before or at compile time. The code generator does not have to meet your low latency goals itself, as it is the final runtime process performance that actually matters. Here is a sample code generation script execution:</p>\n<pre><code class=\"language-bash\">python3 generateCode.py --input inputs.json --output output.cpp\n</code></pre>\n<p>The <strong>inputs.json</strong> file will dictate what will be included in the final code within the <strong>output.cpp</strong> file.</p>\n<h4 id=\"compile-flags-and-templates\">Compile Flags and Templates</h4>\n<p>You can build programs that use C++ template arguments for specialization: both for types and values. Then, at compile time you can supply flags that will select the correct specialization. Everything unrelated to your supplied options will not be included in the runtime. The following is a super simple example:</p>\n<pre><code class=\"language-c++\">#ifdef BUYER\n  using OrderManager = BuyOrderManager;\n#else\n  using OrderManager = SellOrderManager;\n#endif\nSystem&lt;OrderManager&gt; orderManager;\n</code></pre>\n<p>The <code>System</code> class takes <code>OrderManager</code> as a template argument and passes it down to other child members. The value of this template argument changes its runtime behavior at compile time; effectively doing the ‘decision-making’ before the program is executed. Of course, we need to pass in the <code>BUYER</code> flag during compilation:</p>\n<pre><code class=\"language-bash\">g++ -o trade_app trade_app.cpp -DBUYER\n</code></pre>\n<h4 id=\"leveraging-constexpr-utilities\">Leveraging ‘constexpr’ Utilities</h4>\n<p>Modern C++ (since C++11) has given us <code>constexpr</code> which allows marking variables, functions, classes and methods with a hint that expressions can be resolved at compile time. Here are the ‘rules’:</p>\n<ul>\n<li>A constexpr variable must be initialized as a constexpr type with a constepxr function. These must be constructed/called with either literals or other constexpr variables as arguments.</li>\n<li>A constexpr function can only take and return constexpr types. The key thing to know is that arguments can be accepted as constexpr, but not passed on as constexpr. Once within the function you cannot guarantee the argument values are known at compile time, even though they might be.</li>\n<li>A constexpr type/class can only have constexpr type members and must at least one constexpr constructor. All scalar types and arrays are considered constexpr.</li>\n</ul>\n<p>Even with this constrained set of functionality, we can do some pretty fancy stuff. All the way from specializing function bodies using template parameters to parsing JSON at compile time. The rabbit hole goes deep. In a way it is a form of ‘code generation’ included in C++. Other languages have other forms of compile time pre-processing.</p>\n<p>Using <code>constexpr if</code> and a template argument to specialize function body:</p>\n<pre><code class=\"language-c++\">\ntemplate&lt;bool DoOne&gt;\nvoid process() {\n  if constexpr (DoOne) {\n    // ...\n  } else {\n    // ...\n  }\n}\n</code></pre>\n<p>Calculating hash at compile time:</p>\n<pre><code class=\"language-c++\">template&lt;size_t Length&gt;\nconstexpr size_t hash(const char(&amp;str)[Length]) {\n  size_t result = 0;\n  for (size_t i = 0; i &lt; (Length - 1); i++) {\n    result ^= str[i] &lt;&lt; (i % 8);\n  }\n  return result;\n}\n\n// ...\nconstexpr auto hashValue = hash(&quot;Hello, World!&quot;);\n</code></pre>\n<p>Every subsequent version of C++ has significantly boosted the functionality provided by constexpr. Make as much of your code and static data ‘constexpr’ as possible. I have to note something here; The benefit of constexpr is not just the reduction of instructions. You could do that with your own precalculation and storage of static data in an executable written in assembly. This, however, would be incredibly tedious. <code>constexpr</code> is powerful because it treats compile-time resolved data just as it does code; something that can be understood by the programmer and to be updated as objectives/requirements evolve. That is the true power of the constexpr ‘contract’.</p>\n<h2 id=\"external-parallel-processing\">External Parallel Processing</h2>\n<p>Sometimes information is not known before runtime; it requires input from data received during runtime or is updated throughout the day from external sources or computation. This section will discuss a few ways that external processing can be done when the program is already running.</p>\n<h3 id=\"separate-process\">Separate Process</h3>\n<p>Our first option is to have external processes that perform the extracted tasks. This is usually the case for data that does not require inputs from the process itself. Perhaps this is the result of some grid computing batch result or incoming network data. In any case, after the external process is done, it needs a way to pass this data into the already-running main process. The selection of communication method is dependent on data size and the latency requirement. If this is a change that occurs a few times a day we’d use one approach as opposed to a message that is sent every 10 microseconds.</p>\n<p>If this communication rarely happens we can choose a simple method with a low operational overhead such as writing the results to a file and signalling the already running process to read the file. This would be useful for something like a configuration file change or perhaps loading a new machine learning model weights. <strong>SIGUSR1</strong> and <strong>SIGUSR2</strong> are the signals reserved for the program’s own use. The safest way to do this is to register simple flag-setting handler functions at startup which will be called when the signal is received. Handler complexity itself should be kept to a minimum:</p>\n<pre><code class=\"language-c++\">#include &lt;signal.h&gt;\n#include &lt;iostream&gt;\n#include &lt;thread&gt;\n\n// Global variable to indicate if a reload is needed\nbool performReload = false;\n\n// Signal handler\nvoid handler(int) {\n  performReload = true;\n}\n\nint main() {\n  // Register signal handler\n  signal(SIGUSR1, handler);\n\n  // Enter main loop\n  while(true) {\n    if (performReload) {\n      std::cout &lt;&lt; &quot;Reloading configuration...&quot; &lt;&lt; std::endl;\n      performReload = false;\n    }\n    // Actual work\n    std::this_thread::sleep_for(std::chrono::seconds(1));\n  }\n  return 0;\n}\n</code></pre>\n<p>However, if this communication stream is almost constant, we would have to select a different option. For low latency programs, the common choice is using shared memory with atomic access operations. This leverages operating system constructs that allow two different processes to access the same block of memory; something forbidden by default. Atomic operations are instructions which guarantee atomic accesses and updates of memory. In simpler terms; you are able to assume that no other thread/process is able to read/write to the variable while you are reading or writing to it. The other accessor will either get the old version or the new version depending on instruction scheduling and/or atomic sequencing level chosen. Without using atomic operations you might read memory with an in-between state. There are multiple levels of guarantee:</p>\n<ul>\n<li>Relaxed</li>\n<li>Consume</li>\n<li>Release</li>\n<li>Acquire</li>\n<li>Acquire/Release</li>\n<li>Sequentially Consistent</li>\n</ul>\n<p>You can read more about them by reading the documentation of <code>std::atomic::memory_order</code>. In general, as you move down the list, the guarantees get stronger and stronger about how memory is to be written/read as other threads/processes do the same. Really think about how your memory will be accessed and what the reader/writer really needs to be guaranteed in order to fulfill their objectives. Keep in mind: stronger guarantees cost more; time-wise.</p>\n<p>Once you have two processes accessing the same memory space, you can design data structures that enable communication between the two. The favorite choice for low latency applications is a ring buffer implemented as a contiguous array in memory. The API/design differs based on different access scenarios; mainly affected by the number of simultaneous consumers or producers. A single producer/consumer queue is very simple to implement. The following example is not optimized, but captures the general idea. Try to walk through how the reader and writer each would access the memory with atomic and non-atomic instructions. Think about how the access indices interact and why they begin with an offset from one another. After reading about <code>std::atomic::memory_order</code>, which guarantees do we need for which operations? By default, they use <code>memory_order_seq_cst</code>; with the strictest guarantees.</p>\n<p>A simple single producer, single consumer wait-free queue:</p>\n<pre><code class=\"language-c++\">template&lt;typename T, size_t N&gt;\nclass WaitFreeQueue {\n  T _data[N];\n  std::atomic&lt;size_t&gt; _readSequence = 0;\n  std::atomic&lt;size_t&gt; _writeSequence = 1;\npublic:\n  WaitFreeQueue() = default;\n\n  bool tryWrite(T value) {\n    const auto nextWriteIndex = _writeSequence % N;\n    const auto currentReadIndex = _readSequence % N;\n    const bool noRoomLeft = (nextWriteIndex == currentReadIndex);\n    if (noRoomLeft) {\n      return false;\n    }\n    _data[nextWriteIndex] = std::move(value);\n    _writeSequence.store(nextWriteIndex + 1);\n    return true;\n  }\n\n  T* tryRead() {\n    const auto nextReadIndex = (_readSequence + 1) % N;\n    const auto nextWriteIndex = _writeSequence % N;\n    const bool noNewData = (nextReadIndex == nextWriteIndex);\n    if (noNewData) {\n      return nullptr;\n    }\n    _readSequence.store(nextReadIndex);\n    return &amp;_data[nextReadIndex];\n  }\n};\n</code></pre>\n<p>To use this across processes, you would create a shared memory segment and construct this queue in the memory space in <strong>one</strong> process using placement new: <code>new (shared_memory_ptr) WaitFreeQueue&lt;int, 100&gt;();</code>. The other processes would simply interpret the shared memory pointer as the queue and use it.</p>\n<p>If you try to extract the address pointer out of one process and use the raw value in another; your second process would be crashed by the operating system. It would be trying access memory outside its virtual memory space and this is forbidden.</p>\n<h3 id=\"separate-thread\">Separate Thread</h3>\n<p>Sometimes this external computation requires data that the main process provides during runtime. In that case it might be more ergonomic to perform the computation in the same process but on a different thread. For the most part the performance distinction between processes and threads can be blurry. For many operating systems, they operate in the same way once you attach to the same shared memory region. The decision would be more so about the level of coupling you want between the foreground and background task runners. In this section we will be discussing a background task that is: 1. Critical to the runtime of the foreground task. 2. In constant back-and-forth communication with the foreground task. Therefore, it is probably better to use a thread for this job rather than a separate process. This can be summarized into the following:</p>\n<ul>\n<li>Use a process if you want the background task to run uncoupled with the foreground task. I.e. you are okay with each task to run separately.</li>\n<li>Use a thread if you want the background task to be coupled with the foreground task. I.e. you need both tasks to be running constantly.</li>\n</ul>\n<p>One thing to note that is applicable to both separate thread and separate process asynchronous tasks: <strong>The communication mechanism overhead must be less costly to the foreground task then just performing the task itself.</strong> Otherwise, what is the point of doing it in the background?</p>\n<p>Just as with separate processes, using a separate thread to perform tasks will ideally be coordinated using atomic variables as they can allow for ‘wait-free’ operations. You will not be waiting for a non-deterministic amount of time/CPU clock cycles. The same data structures/instructions you use with a separate process can be used across threads.</p>\n<h2 id=\"chapter-summary\">Chapter Summary</h2>\n<ul>\n<li>You can remove expensive computation outside your program to help reduce latency.</li>\n<li>These computations can be done at compile time, at start-up, or in an external thread or process.</li>\n<li>If the processing is done externally, ensure you select the best communication option for the job.</li>\n</ul>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
                   "Low Latency",
                   "C++"
            ],
            "date_published": "2024-03-11T11:39:16-04:00",
            "date_modified": "2024-03-11T11:39:16-04:00"
        },
        {
            "id": "https://tech.davidgorski.ca/introduction-to-low-latency-programming-understand-storage/",
            "url": "https://tech.davidgorski.ca/introduction-to-low-latency-programming-understand-storage/",
            "title": "Introduction To Low Latency Programming: Understand Storage",
            "summary": "This post originally appears as a chapter in my new book: ‘Introduction&hellip;",
            "content_html": "<p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href=\"https://a.co/d/0U6KOfb\">Available now for purchase on Amazon</a>.</em></p>\n<p>So far we have focused on the computation aspect of low latency programming. However, optimized data access plays a huge role in this domain. No process is computation only, it needs data to compute on. This chapter will discuss the key ideas to keep in mind for the optimization of reading and writing from storage.</p>\n<h2 id=\"be-aware-storage-performance-and-access-costs\">Be Aware Storage Performance And Access Costs</h2>\n<p>I am using the word ‘storage’ quite broadly in this chapter; referring to persistent storage, temporary storage, local storage and network storage. From our perspective, we mainly care about ‘time to access’, rather than durability, geographic location or any other ‘implementation’ details.</p>\n<p>It is very important to know the rough time it takes to access different storage sinks. In general, this time increases exponentially as the physical distance from the actual CPU core increases. The following list is ordered from closest to furthest for a theoretical CPU core including the rough ‘time to access’ expressed as a multiplier:</p>\n<ol>\n<li>Registers: 1x</li>\n<li>L1 cache: 5x</li>\n<li>L2 cache: 10x</li>\n<li>L3 cache: 50x </li>\n<li>Same-NUMA memory: 100x</li>\n<li>Different-NUMA memory: 300x</li>\n<li>Persistent Storage (SSD/HDD/Tape): 10000x minimum</li>\n<li>Local Data Center: 200000x</li>\n<li>Distant Data Center: 100000000x</li>\n<li>The Moon: 1000000000000x</li>\n</ol>\n<p>Our closest example storage type is the processor-local set of registers. These are the fastest accessible locations to the processor and are used as inputs and outputs for various computations. Most computers typically operate by loading data from the main memory into these registers, performing some instructions which save results to a register, and finally writing the values from registers into the main memory. Our code is turned into instructions which work with registers. The various levels of cache are optimizations for accessing the main memory.</p>\n<p>The furthest example storage type is a theoretical network drive that is within a computer on the moon. If a program wants to read or write to it, it will have to issue the command over a network spanning 384,400 kilometers. It is obvious that until we develop spacetime wormholes, greater physical distance translates into higher communication latency.</p>\n<p>Keep this list in mind when designing your programs. For low-latency applications, you will ideally never go beyond ‘same-NUMA memory’ after initial start-up is complete. <em>NUMA (Non-Uniform Memory Architecture)</em> is a multiprocessing architecture feature that assigns memory to specific CPU cores in an effort improve overall memory performance. The key takeaway: <em>try to keep your data as close the CPU as possible</em>. Of course, if our program has data persistence requirements, you will factor that in to your design.</p>\n<h2 id=\"caching\">Caching</h2>\n<p>With most modern architectures, continuous memory is loaded together as a group. When a memory address is requested to be read, surrounding data will be pulled into the various cache layers as well. This group of data is called a ‘cache line’. You can see the size of the cache line on your platform with the <strong>std::hardware_destructive_interference_size</strong> value in C++.</p>\n<p>Keeping cache mechanics in mind is important as you design your algorithms and data structures. Factoring them into your program implementation can greatly increase performance and therefore reduce latency. In general, to keep a portion of code running as fast as possible, have it access as small a range of continuous memory as possible.</p>\n<p>Put data that will be accessed as ‘a unit’ as close as possible; ideally within one cache-line for maximum performance. There are still huge benefits for using continuous memory beyond that. For example, adjacent cache lines are often prefetched. Generally use data structures that are as flat as possible (in contiguous memory) like static or dynamic arrays. They should be the default data structure of choice. Additionally, design your algorithms to fully process a block of contiguous memory before moving on (avoid going back). Sometimes this means re-arranging the order of some loops, such as with this simple matrix multiplication function:</p>\n<p>Before:</p>\n<pre><code class=\"language-c++\">using Vector = std::vector&lt;float&gt;;\nusing TwoDimMatrix = std::vector&lt;Vector&gt;;\n\nTwoDimMatrix matMul(const TwoDimMatrix&amp; m1, const TwoDimMatrix&amp; m2)\n{\n  const auto sharedDim = m1.front().size();\n  TwoDimMatrix result(m1.size(), Vector(m2.front().size()));\n  // Iterate over rows of m1\n  for (size_t i = 0; i &lt; m1.size(); ++i) {\n    // Iterate over rows of m2\n    for (size_t j = 0; j &lt; m2.size(); ++j) {\n      // Iterate over cells of m1 row and m2 column\n      for (size_t k = 0; k &lt; sharedDim; ++k) {\n        result[i][j] += m1[i][k] * m2[k][j];\n      }\n    }\n  }\n  return result;\n}\n</code></pre>\n<p>After:</p>\n<pre><code class=\"language-c++\">TwoDimMatrix matMul(const TwoDimMatrix&amp; m1, const TwoDimMatrix&amp; m2)\n{\n  const auto sharedDim = m1.front().size();\n  TwoDimMatrix result(m1.size(), Vector(m2.front().size()));\n  // Iterate over rows of m1\n  for (size_t i = 0; i &lt; m1.size(); ++i) {\n    // Iterate over cells of the m1 row and rows of m2\n    for (size_t k = 0; k &lt; sharedDim; ++k) {\n      // Iterate over cells of the m2 row\n      for (size_t j = 0; j &lt; m2.size(); ++j) {\n        result[i][j] += m1[i][k] * m2[k][j];\n      }\n    }\n  }\n  return result;\n}\n</code></pre>\n<p>All we really did was switch the k and j loop nesting order. This change can yield up to a 10x speed up as iterating over a column is much more expensive than iterating through a row. Why? The row is continuous in memory, whereas the column is not. Every time we access the next value in the column we are actually accessing a relatively ‘distant’ part of memory.</p>\n<p>Another thing to keep in mind is ‘false sharing’. If you are using atomic instructions on two memory locations that are within a single cache line, you could inadvertently slow down your program as the CPU will send an instruction to lock the cache line from other cores. This is unavoidable and desired if the two cores are actually accessing the same memory location, but if it just happens to be nearby and not the <em>same exact</em> memory location, we are paying a price for nothing. To mitigate this, you want to make sure the two memory addresses are on separate cache lines using the C++ <code>alignas</code> keyword:</p>\n<pre><code class=\"language-c++\">struct DoNotShare {\n  alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; one;\n  alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; two;\n};\n</code></pre>\n<p>This will prevent any false sharing from occurring by aligning the variables to offsets defined by the integer value returned by <strong>std::hardware_destructive_interference_size</strong>. Keep this in mind when using atomic instructions on nearby memory addresses.</p>\n<h2 id=\"chapter-summary\">Chapter Summary</h2>\n<ul>\n<li>Know the rough time taken to access different storage sinks. From same-NUMA memory cache line all the way to a computer sending RF signals from the moon.</li>\n<li>Understand the cache characteristics of your hardware.</li>\n<li>Keep in mind that that measurement is key; educated assumptions can be useful early on in the design process, but to derive conclusions you must measure performance yourself.</li>\n</ul>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
                   "Low Latency",
                   "C++"
            ],
            "date_published": "2024-03-04T12:01:37-05:00",
            "date_modified": "2024-03-04T12:01:37-05:00"
        },
        {
            "id": "https://tech.davidgorski.ca/introduction-to-low-latency-programming-minimize-branching-and-jumping/",
            "url": "https://tech.davidgorski.ca/introduction-to-low-latency-programming-minimize-branching-and-jumping/",
            "title": "Introduction To Low Latency Programming: Minimize Branching And Jumping",
            "summary": "This post originally appears as a chapter in my new book: ‘Introduction&hellip;",
            "content_html": "<p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href=\"https://a.co/d/0U6KOfb\">Available now for purchase on Amazon</a>.</em></p>\n<p>This chapter will discuss how branching and jumping in our code affects our runtime performance and how we can avoid them in our effort to reduce the latency of our programs. <strong>Branching</strong> refers to process execution that can go down one of multiple paths. This functionality is provided by ‘check and jump’ instructions. <strong>Jumping</strong> refers to when the program control pointer is changed to a different location in memory rather than progressing to the next sequential instruction. This can be conditional (as a part of branching) or unconditional.</p>\n<h2 id=\"what-does-it-cost\">What Does It Cost?</h2>\n<p>Why do we want to discourage Jumping and/or Branching? Firstly, ‘comparing and jumping’ simply adds more instructions. As mentioned throughout the book, we want to minimize instructions overall. Secondly, branching disrupts the smooth flow of instruction prefetching, meaning if a condition that was not guessed by the branch predictor is encountered, the instruction pipeline has to potentially be flushed and refreshed. A new sequence of instructions will have to be retrieved.</p>\n<p>The branch predictor is an element of CPUs that aims to predict which branch will be taken in our code during runtime. Branch predictors have gotten more and more complex, but the fundamental idea is maintaining branching statistics and using them to determine where a program control address branch will most likely jump to. As mentioned in the previous paragraph, that prediction allows the pre-fetch of upcoming instructions.</p>\n<p>More instructions and instruction pipeline invalidation both contribute to the stalling of program execution. And stalling means we are not running the important instructions that directly contribute to fulfilling the business objectives. In an ideal world, our program would run these ‘golden instructions’ uninhibited.</p>\n<h2 id=\"examples-of-branching-and-jumping\">Examples Of Branching And Jumping</h2>\n<p>The following short list contains <em>some</em> common code constructs which will generate jumping and/or branching instruction constructs:</p>\n<ul>\n<li><strong>Branching and Jumping</strong>: If statements, Chained boolean conditionals, Virtual method calls</li>\n<li><strong>Jumping:</strong> Function calls</li>\n</ul>\n<h2 id=\"steps-to-reduce-branchingjumping\">Steps To Reduce Branching/Jumping</h2>\n<h3 id=\"branchless-boolean-expressions\">Branchless Boolean Expressions</h3>\n<p>Standard boolean expression operations such as <code>&amp;&amp;</code> and <code>||</code> have ‘short-circuiting’ as a feature; if previous boolean expressions evaluate to false, consecutive expressions will not be evaluated:</p>\n<pre><code class=\"language-c++\">const bool andExpression =\n  false /*will be evaluated*/ &amp;&amp;\n  true /* will not be evaluated*/;\nconst bool orExpression =\n  true /*will be evaluated*/ ||\n  false /* will not be evaluated*/;\n</code></pre>\n<p>As expected, this introduces branching to our code and should be considered for removal. What do we use instead? You can use the bitwise AND and OR expressions:</p>\n<pre><code class=\"language-c++\">const bool andExpression = false &amp; true; // Both will be evaluated\nconst bool orExpression = true | false; // Both will be evaluated\n</code></pre>\n<p>When should you switch to this approach? There is a simple rule of thumb: switch from logical to bitwise boolean expressions when the overhead cost of branching does not outweigh the cost of evaluating all terms in the expression.</p>\n<p>It still makes sense to use logical boolean expressions if consecutive expressions are expensive to evaluate. Such the following:</p>\n<pre><code class=\"language-c++\">const bool result = cheapCheck() &amp;&amp; expensiveCheck();\n</code></pre>\n<p>The short-circuiting branch allows us to avoid the expensive check, which the branchless, bitwise version would not. Tip: For the logical flavor, it makes sense to arrange the expressions in a chain from least to most expensive to enable short-circuiting before the largest computation costs would be incurred.</p>\n<p>Take a good look at the cost of the expression terms before deciding on using the branching or branchless boolean expressions.</p>\n<h3 id=\"write-code-which-generates-cmov-like-instructions\">Write Code Which Generates ‘CMOV’-like instructions</h3>\n<p><code>CMOV</code> is a built-in instruction on the x86 architecture which does a ‘conditional move (copy)’. While spiritually similar to a branch, it is significantly cheaper than real ‘check and jump’. Since we are not writing assembly by hand, we won’t be directly implementing <code>CMOV</code> use. Rather, we will try to use code constructs that will be turned into <code>CMOV</code> instructions. One of these is ternary operator. Regardless, a good compiler will use <code>CMOV</code> when it can, even for if statements (when they simply assign to a value to a variable).</p>\n<p>Ternary operator use:</p>\n<pre><code class=\"language-c++\">const int output = useFortyTwo() ? 42 : 24;\n</code></pre>\n<p>Make sure to measure the actual effect of using <code>CMOV</code> instructions with your program and data. Sometimes well-predicted branches can be more performant than <code>CMOV</code>.</p>\n<h3 id=\"remove-the-use-of-virtual-functions\">Remove The Use Of Virtual Functions</h3>\n<p>Virtual function calls perform a jump. When it comes time to call the virtual function, the generated virtual lookup table is consulted for that class. Based on the values in the table, the correct implementation of the method will be ‘jumped to’ to in the executable. This incurs a cost to load the vtable as well a branching jump. Ideally, we will avoid virtual function calls in our programs. If you are using purely virtual classes as a form of interface, you can consider using C++20 concepts instead. They will allow you to constrain generic code and allow you to create ‘interfaces’ via <code>requires</code>.</p>\n<p>Of course, for leveraging dynamic dispatch (runtime specified code execution) there is no alternative to runtime branching / jumping. Other approaches such as using sum types (<code>std::variant</code>) or type erasure all perform the same relative types of instruction. Think about if you really need dynamic dispatch.</p>\n<h3 id=\"inline-your-functions\">Inline Your Functions</h3>\n<p>Function inlining is another feature we can leverage to reduce jumps. In addition to jumping to a different location in our code and dealing with the costs of that, we also don’t have to pay the function call overhead. This overhead includes saving the register states, filling registers and/or the stack with the function arguments, incrementing the stack pointer, saving the stack return location and saving the current program control location. Function inlining puts a copy of the function body being called right at the call site. This can result in some great performance gains as there is no jump and the instruction pipeline is as simple as possible and spatially localized. You can use the GCC <code>always_inline</code> attribute above functions you want to be strictly inlined. For example:</p>\n<pre><code class=\"language-c++\">[[gnu::always_inline]\nint function(int a) {\n  return a + 11232;\n}\n</code></pre>\n<p>Even though this function would have most likely already been inlined due to its simplicity, this attribute will forcefully encourage the compiler to do so, provide a warning to us if it is unable to, and also encodes the inlining objective within the code itself for all programmers to see.</p>\n<h2 id=\"compiler-hints\">Compiler Hints</h2>\n<p>If branching is absolutely necessary in your code, you can use compiler hints to provide the compiler with extra information that it cannot infer. Using this information, the compiler will potentially re-order instructions in a way that will make the branch you marked as more ‘likely’ to be more efficient from a conditional branching perspective. This can be done using the <code>__builtin_expect</code> function or from C++20 and above with the <code>likely</code> and <code>unlikely</code> attributes. You can make the <code>__builtin_expect</code> function more ergonomic to use by creating <code>LIKELY</code> and <code>UNLIKELY</code> macros:</p>\n<pre><code class=\"language-c++\">#define LIKELY(x) __builtin_expect(!!(x), 1)\n#define UNLIKELY(x) __builtin_expect(!!(x), 0)\n</code></pre>\n<p>Let’s explore a simple use case and it’s effects. Take a look at this simple function, and it’s corresponding assembly:</p>\n<p>C++:</p>\n<pre><code class=\"language-c++\">void func(int input) {\n  if (input == 1) {\n    // BRANCH ONE\n  } else if (input == 2 ) {\n    // BRANCH TWO\n  } else {\n    // BRANCH THREE\n  }\n}\n</code></pre>\n<p>Assembly:</p>\n<pre><code class=\"language-asm\">func(int): # @func(int)\n  movq %rdi, %rax\n  leaq 1(%rdi), %rcx\n  cmpl $2, %esi\n  je .LBB0_3\n  cmpl $1, %esi\n  jne .LBB0_4\n  # BRANCH ONE\n  retq\n.LBB0_3:\n  # BRANCH TWO\n  retq\n.LBB0_4:\n  # BRANCH THREE\n  retq\n</code></pre>\n<p>Notice that the <code>cmpl</code> instruction is first executed with ‘2’ as the argument. Only after, if we didn’t jump, it will execute with ‘1’. What if we know that the value in <code>$esi</code> is more likely to be equal to ‘1’ and we want to prioritize that branch? Let’s add a <code>LIKELY</code> annotation to our C++ code:</p>\n<pre><code class=\"language-c++\">void func(int input) {\n  if (LIKELY(input == 1)) {\n    // BRANCH ONE\n  } else if (input == 2 ) {\n    // BRANCH TWO\n  } else {\n    // BRANCH THREE\n  }\n}\n</code></pre>\n<p>Now we can observe the different in output assembly:</p>\n<pre><code class=\"language-asm\">func(int): # @func(int)\n  pushq %rbx\n  cmpl $1, %esi\n  jne .LBB0_1\n  # BRANCH ONE\n  retq\n.LBB0_1:\n  cmpl $2, %esi\n  jne .LBB0_4\n  # BRANCH TWO\n  retq\n.LBB0_4:\n  # BRANCH THREE\n  retq\n</code></pre>\n<p>As you can see, the compiler re-arranged the branching instructions. The comparison of <code>%eri</code> with 1 is now part of the first call to <code>cmpl</code> + <code>jne</code> and if equality is determined we don’t jump anywhere else. Spatially, we are already at the <code>BRANCH ONE</code> code. The compiler also moved the remaining conditionals after the first jump at <code>.LBB0_1:</code>. It is quite apparent that all remaining cases are deprioritized from an optimization standpoint in comparison to <code>BRANCH ONE</code>.</p>\n<p>Now for some semantic rambling. Even though they have been accepted as standard terms, likely and unlikely are not really accurate from a definition perspective. You are not really marking that one conditional branch is more likely than the other to occur. What you are doing is telling the compiler that you want to optimize for this conditional branch. Perhaps <code>OPTIMIZE</code> and <code>UNOPTIMIZE</code> are more accurate terms for our construct. Making your code as readable/understandable/surface-level as possible is always something worth pursuing.</p>\n<h2 id=\"cheaper-branching\">Cheaper Branching</h2>\n<p>Not all ‘branching’ instructions are created equally. Consider the following function that that performs a few <strong>compare and jumps</strong>:</p>\n<pre><code class=\"language-c++\">void processThisString(std::string_view input)\n{\n  if (input == &quot;production&quot;) {\n    processProd(input);\n  } else if (input == &quot;RC&quot;) {\n    processRC(input);\n  } else if (input == &quot;beta&quot;)\n    processBeta(input);\n  }\n}\n</code></pre>\n<p>This is a ‘run-of-the-mill’ if statement. However, we have an opportunity here: the comparison set is very constrained. In fact, most the calls to the <code>==</code> operator distill to a <code>memcmp</code> which will short circuit after the first character, since all the first characters in the string literals are different. For example, if the input’s value is “beta”, we will perform two calls to <code>==</code> for nothing. Instead, we can re-write this in a more efficient manner by using the <code>switch</code> construct on that first character:</p>\n<pre><code class=\"language-c++\">void processThisString(std::string_view input)\n{\n  constexpr auto i = 0;\n  switch (input[i]) {\n    case &quot;production&quot;[i]: processProd(input); break;\n    case &quot;RC&quot;[i]: processRC(input); break;\n    case &quot;beta&quot;[i]: processBeta(input); break;\n  }\n}\n</code></pre>\n<p>The compiler will most likely generate a more efficient branching construct using jump tables and binary decision trees since data of only 1 char width is being compared now. A best-case scenario would be if the range of first character values is very limited, because then a single jump table would be used to increment the program control pointer. Of course, this is not cut and dry, because a small set of inputs may cause an if statement to perform better. Always measure! Regardless, this is a good approach to keep in mind for optimization.</p>\n<h2 id=\"chapter-summary\">Chapter Summary</h2>\n<ul>\n<li>Jumping to distant code locations or branching can incur a heavy cost in low latency segments of the code. This is due to added instructions, instruction pipeline de-optimization and cache eviction.</li>\n<li>Examples of Branching/Jumping include: if statements, chained conditionals, virtual method calls, and function calls in general.</li>\n<li>You can avoid branching/jumping by using branchless conditionals, Using ‘CMOV’ instructions, Remove the use of virtual functions and try to inline your functions.</li>\n<li>If branching is unavoidable, try to use a ‘cheaper’ variation of it.</li>\n</ul>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
                   "Low Latency",
                   "C++"
            ],
            "date_published": "2024-02-26T15:42:03-05:00",
            "date_modified": "2024-02-26T15:42:03-05:00"
        },
        {
            "id": "https://tech.davidgorski.ca/announcing-my-book-introduction-to-low-latency-programming/",
            "url": "https://tech.davidgorski.ca/announcing-my-book-introduction-to-low-latency-programming/",
            "title": "Announcing My First Book: Introduction To Low Latency Programming",
            "summary": "I’m excited to announce the availability of my first technical book: Introduction&hellip;",
            "content_html": "<p>I’m excited to announce the availability of my first technical book:</p>\n<p><strong>Introduction To Low Latency Programming</strong> </p>\n<p>Amazon Link: <a href=\"https://a.co/d/0U6KOfb\">https://a.co/d/0U6KOfb</a></p>\n<p>It is a short, beginner-friendly entry into the domain of high-performance, bare metal coding. Learning low latency programming can be intimidating and online resources are scattered. Since there are no introductory books on the subject, my hope is that I can help programmers ‘kick start’ their entry into the domain gently. While this the book is not ‘deep’, it touches on important technical topics and ideas, provides many code examples and discusses the effects of modern hardware on optimization problems.</p>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
            ],
            "date_published": "2024-02-23T11:12:14-05:00",
            "date_modified": "2024-02-23T11:13:38-05:00"
        },
        {
            "id": "https://tech.davidgorski.ca/literary-programming/",
            "url": "https://tech.davidgorski.ca/literary-programming/",
            "title": "Literary Programming",
            "summary": "Code is written once, but read many times. Given this truth, it&hellip;",
            "content_html": "<p>Code is written once, but read many times. Given this truth, it makes sense to prioritize read-ability over write-ability. Readable code contains good variable names, uses appropriate abstractions and is understood easily. Many have waved the flag of ‘Clean Code’ for quite a few decades now.</p>\n<p>What has helped me write code that I consider clear and readable is something I call <em>Literary Programming</em>. Not to be confused with <a href=\"https://en.wikipedia.org/wiki/Literate_programming\">Literate Programming</a>. I would change the name of my concept but I’ve grown rather fond of it. Apologies in advance.</p>\n<p>What I call <em>Literary Programming</em> is the deliberate application of structured grammar and story-telling into the naming, abstracting and composition of code. It is the conscious act of letting your code capture a narrative with your choice of words. Not leaving intentions and actions in a state of vagueness; method names should reflect what is actually being done within and the meaning of the return value, intermediate variables are encouraged.</p>\n<p>Don’t overanalyze what I am trying to say. This isn’t a very strict set of rules. It may just be synonymous with ‘Good Code’ to you. It is a simple idea meant to inspire a little more scrutiny in you when creating APIs, choosing names, composing expressions and designing the structure of your application.</p>\n<p>Here are just some ways you can make your more <strong>literary</strong>:</p>\n<ul>\n<li>Use abstractions to ‘shorten’ portions of code. Simple example: putting argument and configuration parsing in a separate function to keep the rest of the <code>main</code> as clean as possible.</li>\n<li>Put great care into class API design; as much detail as possible into method names. Prefer a name like <strong>processIncomingMessages()</strong> over <strong>process()</strong> or <strong>messages()</strong>. <strong>getOrCreateItem()</strong> over <strong>getItem()</strong> or <strong>createItem()</strong> for a method that will return an existing object if it already exists.</li>\n<li>Value consistency. One method shouldn’t be named <strong>bool error()</strong> while it’s counterpart is named <strong>bool isValid()</strong>.</li>\n<li>Use intermediate variables to compose expressions. These variables serve as logic checkpoints for the future reader.</li>\n<li>Wrap complicated loop conditions into a boolean-returning functor. Take this call site as an example: <code>while (continueRunning(itemsLeft, maxItemsToProcess, error))</code>.</li>\n<li>As mentioned before; good naming pays dividends.</li>\n</ul>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
                   "style",
                   "Programming"
            ],
            "date_published": "2024-02-09T16:41:04-05:00",
            "date_modified": "2024-02-09T16:44:51-05:00"
        },
        {
            "id": "https://tech.davidgorski.ca/c-mini-pattern-deriving-from-stdvariant/",
            "url": "https://tech.davidgorski.ca/c-mini-pattern-deriving-from-stdvariant/",
            "title": "C++ Pattern: Deriving From std::variant",
            "summary": "I am a big fan of sum types for expressive programming. They&hellip;",
            "content_html": "<p>I am a big fan of sum types for expressive programming. They provide an elegant way to encode mutually exclusive data types in a single field. While not provided by the language itself, the C++ standard library offers us <code>std::variant</code>. Since there is no language-level pattern matching construct, interacting with variants can be less than ergonomic. One way to mitigate this is inheriting from <code>std::variant</code> and creating useful domain-specific access methods. This article discusses a few different ways of deriving from <code>std::variant</code> that might be useful and/or interesting.</p>\n<h2 id=\"a-result-type\">A Result Type</h2>\n<p>To start off we’ll create a ~15 line class derived from <code>std::variant</code> that fulfills the basics of a result type (something like C++23 <code>std::expected</code> but available in C++17 and above). This is a nice way to encapsulate success and failures types in a united interface. Implementation:</p>\n<pre><code class=\"language-c++\">#include &lt;variant&gt;\n\ntemplate&lt;typename T, typename Error&gt;\nclass Result : public std::variant&lt;T, Error&gt; {\npublic:\n  using std::variant&lt;T, Error&gt;::variant;\n  using std::variant&lt;T, Error&gt;::operator=;\n  \n    Error* error() { return std::get_if&lt;Error&gt;(this); }\n    const Error* error() const { return std::get_if&lt;Error&gt;(this); }\n    T* value() { return std::get_if&lt;T&gt;(this); }\n    const T* value() const { return std::get_if&lt;T&gt;(this); }\n    T* operator-&gt;() { return value(); }\n    const T* operator-&gt;() const { return value(); }\n    operator bool() const { return error() == nullptr; }\n};\n</code></pre>\n<p>We derive from <code>std::variant</code> to take advantage of the ergonomic constructors and assignment operators (meaning we don’t have to implement them for each type and each reference type; doing this correctly is tedious). After this, we add two convenient methods to access the underlying types (along with const overloads). And finally, define <code>operator-&gt;</code> to allow access to the underlying success type value and <code>operator bool</code> to determine whether it holds the success type (along with const overloads).</p>\n<p>To demonstrate basic usage we can create a small type hierarchy with success and failure types:</p>\n<pre><code class=\"language-c++\">struct SuccessResult {\n  int date;\n  double time;\n};\nstruct ErrorResult {\n  std::string message;\n};\nusing ProcessResult = Result&lt;SuccessResult, ErrorResult&gt;;\n</code></pre>\n<p>Then we create a function to demonstate it’s usage:</p>\n<pre><code>ProcessResult process(std::string_view input) {\n  return ErrorResult{ &quot;Not implemented&quot; };\n}\n</code></pre>\n<p>And finally we call the function and write some code that observes the return value:</p>\n<pre><code>const auto result = process(&quot;Hello&quot;);\nif (!result) {\n  std::cout &lt;&lt; &quot;Error: &quot; &lt;&lt; result.error()-&gt;message &lt;&lt; std::endl;\n} else {\n  std::cout &lt;&lt; &quot;Date: &quot; &lt;&lt; result-&gt;date &lt;&lt; &quot; Time: &quot; &lt;&lt; result-&gt;time &lt;&lt; std::endl;\n}\n</code></pre>\n<p>Overall, this approach is highly ergonomic, concise and clear. The call site is easily understood since <code>operator bool</code> cleanly checks the status, <code>operator -&gt;</code> eliminates unnecessary intermediate variables and method calls. The <code>error()</code> access method is also self-explanatory.</p>\n<h2 id=\"data-or-pointer-to-data\">Data Or Pointer to Data</h2>\n<p>Another potentially useful class we can build is <code>DataOrPointer</code>. This is a class that either holds a type or a pointer to that type. Again we provide ergonomic access methods and operators:</p>\n<pre><code class=\"language-c++\">template&lt;typename T&gt;\nclass DataOrPointer : public std::variant&lt;T, T*&gt; {\npublic:\n  using std::variant&lt;T, T*&gt;::variant;\n  using std::variant&lt;T, T*&gt;::operator=;\n  operator const T&amp;() const {\n    if (auto value = std::get_if&lt;T&gt;(this); value) {\n      return *value;\n    }\n    return *std::get&lt;T*&gt;(*this);\n  }\n};\n</code></pre>\n<p>You can use this if you want to provide a single return type to a function that takes a <code>T</code> as an argument and may or may not return a newly constructed <code>T</code> object after testing some conditions. If constructing <code>T</code> is expensive, this approach can be a clean way to achieve the objective. A real-world example of this could be normalizing two BigFloats to use the same exponent before operating on them. If the target exponent is equal to the current exponent, it would be a waste to build a new copy (In this case we can’t mutate the original numbers).</p>\n<pre><code>DataOrPointer&lt;const UnsignedBigFloat&gt; usingExponent(const UnsignedBigFloat&amp; value, int64_t exponent)  {\n    if (exponent == value._exponent) {\n        return DataOrPointer&lt;const UnsignedBigFloat&gt;(&amp;value);\n     }\n\n    auto copy = value;\n    copy._mantissa.timesTenToThe(exponent - value._exponent);\n    copy._exponent = exponent;\n    return std::move(copy);\n}\n</code></pre>\n<h2 id=\"multi-type-reference\">Multi-Type Reference</h2>\n<p>I will admit the following example is almost too esoteric to be useful, but I have actually reached for this once before.</p>\n<p>The challenge: create a reference that can bind to one of multiple types, performance not being critical and reducing code duplication being the main objective.</p>\n<p>Solution:</p>\n<pre><code class=\"language-c++\">template&lt;typename... Ts&gt;\nclass MultiTypeReference : public std::variant&lt;Ts*...&gt; {\npublic:\n  using std::variant&lt;Ts*...&gt;::variant;\n\n  template&lt;typename T&gt;\n  auto&amp; operator=(T value) {\n    std::visit([&amp;](auto&amp; pointer) { *pointer = value; }, *this);\n    return *this;\n  }\n\n  template&lt;typename T&gt;\n  operator T() {\n    return std::visit([&amp;](auto&amp; pointer) { return T(*pointer); }, *this);\n  }\n</code></pre>\n<p>Yes, this works. Yes, it’s weird. No, I don’t encourage you to use it. However, it is an interesting case study and hopefully gets you thinking about how to stretch the use of <code>std::variant</code>. Here’s how one would actually use it:</p>\n<pre><code>struct Data {\n   bool useFieldA;\n   int32_t A;\n     int64_t B;\n};\n\nMultiTypeReference&lt;int32_t, int64_t&gt; getRelevantField(Data&amp; data) {\n    return data.useFieldA ?\n          MultiTypeReference&lt;int32_t, int64_t&gt;(data.A) :\n          MultiTypeReference&lt;int32_t, int64_t&gt;(data.B);\n}\n\nvoid assignOne(Data&amp; one) {\n  getRelevantField(one) = 1;\n}\n</code></pre>\n<p>At the end of the day, yes, this simply hides the conditional access and assignment behind some abstractions. But in use, it behaves exactly how we want it: a reference to one of multiple types.</p>\n<h2 id=\"epilogue\">Epilogue</h2>\n<p>The reason I included the word ‘Pattern’ in the title is because these ideas can be extended to theoretically endless types and custom access methods.</p>\n<p>For examples, you could build a result type with three different possible types and provide access methods for them (removing <code>operator-&gt;</code>). Or perhaps you provide a <code>transform</code> method that takes a functor and changes the value to hold a different type after transforming the current type. Or even just provide custom comparison operators (Which could be necessary for sorting different numeric types; imagine you want to sort a vector of regular int and BigInt references stored within a variant-type).</p>\n<p>In review, the key tools to leverage are:</p>\n<ol>\n<li>Using the constructors and assignment operators provided by <code>std::variant</code>.</li>\n<li>Defining named access methods for different types.</li>\n<li>Providing an <code>operator-&gt;</code> for a success or special type.</li>\n<li>Providing conversion operators for syntax-less unpacking.</li>\n</ol>\n<p>I hope my discussion of these concepts was useful or at least interesting. Thanks for reading! Subscribe via RSS or <a href=\"https://www.linkedin.com/in/dgski/\">LinkedIn</a>.</p>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
            ],
            "date_published": "2024-01-09T22:00:00-05:00",
            "date_modified": "2024-01-31T20:59:35-05:00"
        },
        {
            "id": "https://tech.davidgorski.ca/c-iterator-friendly-branchless-binary-search/",
            "url": "https://tech.davidgorski.ca/c-iterator-friendly-branchless-binary-search/",
            "title": "C++ Iterator-Friendly Branchless Binary Search",
            "summary": "Once you delve into the realm of low-latency C++, you will find&hellip;",
            "content_html": "<p>Once you delve into the realm of low-latency C++, you will find yourself waking up in the middle of the night, sweating profusely from a nightmare concerning unnecessary branching. And soon after, you begin to over-optimize your code to avoid branches. Even when that part of your code is clearly not the bottleneck.</p>\n<p>This is a short post presenting a C++ iterator-friendly implementation of a branchless binary search implementation. It is short and sweet, so I will reveal it before some editorial comments and thanks:</p>\n<pre><code>template&lt;typename It&gt;\nIt lower_bound(It begin, It end, const typename It::value_type&amp; value) {\n  auto len = std::distance(begin, end);\n  if (len == 0) {\n    return end;\n  }\n\n  while (len &gt; 1) {\n    const auto half = len / 2;\n    begin += (*(begin + half - 1) &lt; value) * half;\n    len -= half;\n  }\n  return (*begin &lt; value) ? end : begin;\n}\n</code></pre>\n<p>This implementation uses a <code>begin</code> iterator and <code>len</code> integer to keep track of the search space rather than begin and end pointers/indices. So the first line of the function are simply using the provided iterators to derive the needed variables. We terminate early if the range is empty. All classic binary search branches within the <code>while</code> loop body have been removed:</p>\n<ul>\n<li>Shrinking the search space length without branches is easy; it will always halve. We can do this safely by subtracting <code>halfLen</code> from the remaining <code>len</code> (This will potentially leave an array of size 1).</li>\n<li>Calculating the new search space start is more tricky. It will either be the same <code>begin</code> as now, or be the halfway point between pos and the end of the search space. So we conditionally add <code>halfLen</code> to <code>begin</code>.</li>\n<li>To conform to the <code>std::lower_bound</code> interface, we must return the end iterator if the value is not found within the range. To do so, unfortunately we must add a final conditional at the end of the function. Thanks to <a href=\"https://www.linkedin.com/in/farid-mehrabi/\">Farhid Mehrabi</a> for pointing this bug out in my initial implementation (I would potentially return a value that is less than the value, breaking the lower bound contract). Adding a simple unit test would have prevented my from doing so, so this a reminder to myself to do so even for small pieces of code.</li>\n</ul>\n<p>For the most part, this implementation will have superior performance to <code>std::lower_bound</code>. There is caveat: since the addition to <code>begin</code> will most likely be turned into a <code>CMOV</code> instruction rather than a proper branch, there will be no prediction to preload the next search space mid-points and at larger array sizes, a classic approach will prevail. This can potentially be mitigated on some platforms by adding an explicit <code>prefetch</code> instruction.</p>\n<p>Thanks to <a href=\"https://en.algorithmica.org/hpc/data-structures/binary-search/\">this amazing article</a> for outlining this concept using raw arrays.</p>\n<h2 id=\"benchmarks\">Benchmarks</h2>\n<p>Nanoseconds taken to find 1000 random numbers that are in the range on Macbook Air M1. <a href=\"https://gist.github.com/dgski/c48142bcb96cc3bf0bf14fe1072e403f\">Code</a>.</p>\n<pre><code>=============================\narray size = 1\nbranchlessTime = 18000\nstdTime        = 18000\n=============================\narray size = 2\nbranchlessTime = 18000\nstdTime        = 30000\n=============================\narray size = 4\nbranchlessTime = 19000\nstdTime        = 39000\n=============================\narray size = 8\nbranchlessTime = 19000\nstdTime        = 48000\n=============================\narray size = 16\nbranchlessTime = 20000\nstdTime        = 59000\n=============================\narray size = 32\nbranchlessTime = 21000\nstdTime        = 69000\n=============================\narray size = 64\nbranchlessTime = 21000\nstdTime        = 78000\n=============================\narray size = 128\nbranchlessTime = 22000\nstdTime        = 91000\n=============================\narray size = 256\nbranchlessTime = 24000\nstdTime        = 103000\n=============================\narray size = 512\nbranchlessTime = 28000\nstdTime        = 113000\n=============================\narray size = 1024\nbranchlessTime = 31000\nstdTime        = 127000\n=============================\narray size = 2048\nbranchlessTime = 35000\nstdTime        = 139000\n=============================\narray size = 4096\nbranchlessTime = 38000\nstdTime        = 146000\n=============================\narray size = 8192\nbranchlessTime = 42000\nstdTime        = 140000\n=============================\narray size = 16384\nbranchlessTime = 40000\nstdTime        = 139000\n=============================\narray size = 32768\nbranchlessTime = 45000\nstdTime        = 149000\n=============================\narray size = 65536\nbranchlessTime = 64000\nstdTime        = 189000\n=============================\narray size = 131072\nbranchlessTime = 79000\nstdTime        = 216000\n=============================\narray size = 262144\nbranchlessTime = 91000\nstdTime        = 233000\n=============================\narray size = 524288\nbranchlessTime = 105000\nstdTime        = 260000\n=============================\narray size = 1048576\nbranchlessTime = 195000\nstdTime        = 351000\n=============================\narray size = 2097152\nbranchlessTime = 142000\nstdTime        = 333000\n=============================\narray size = 4194304\nbranchlessTime = 397000\nstdTime        = 444000\n=============================\narray size = 8388608\nbranchlessTime = 931000\nstdTime        = 961000\n=============================\narray size = 16777216\nbranchlessTime = 1159000\nstdTime        = 996000\n=============================\narray size = 33554432\nbranchlessTime = 1327000\nstdTime        = 1132000\n=============================\narray size = 67108864\nbranchlessTime = 1507000\nstdTime        = 1578000\n=============================\narray size = 134217728\nbranchlessTime = 1603000\nstdTime        = 1710000\n=============================\narray size = 268435456\nbranchlessTime = 5921000\nstdTime        = 5980000\n=============================\narray size = 536870912\nbranchlessTime = 23558000\nstdTime        = 15002000\n</code></pre>\n<p>As you can see the benefits start to deteriorate as the array size grows.</p>\n",
            "author": {
                "name": "David Gorski"
            },
            "tags": [
            ],
            "date_published": "2023-12-17T14:32:39-05:00",
            "date_modified": "2023-12-18T09:36:35-05:00"
        }
    ]
}
