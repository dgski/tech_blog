<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Tech @ DG</title>
    <link href="https://tech.davidgorski.ca/feed.xml" rel="self" />
    <link href="https://tech.davidgorski.ca" />
    <updated>2024-03-18T11:09:52-04:00</updated>
    <author>
        <name>David Gorski</name>
    </author>
    <id>https://tech.davidgorski.ca</id>

    <entry>
        <title>Introduction To Low Latency Programming: Clarify Program Scope</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/introduction-to-low-latency-programming-clarify-program-scope/"/>
        <id>https://tech.davidgorski.ca/introduction-to-low-latency-programming-clarify-program-scope/</id>
            <category term="Programming"/>
            <category term="Low Latency"/>
            <category term="C++"/>

        <updated>2024-03-18T11:09:52-04:00</updated>
            <summary>
                <![CDATA[
                    This post originally appears as a chapter in my new book: ‘Introduction&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href="https://a.co/d/0U6KOfb">Available now for purchase on Amazon</a>.</em></p>
<p>I would like to begin this chapter by referencing a quote: “Measure twice, cut once.” While this statement is almost a platitude, it can be a useful reminder for careful, deliberate planning before taking action. I think taking this idea to heart is very important when it comes to low latency programming. The earlier in the development process you include low latency objectives, the better your software design will be and the better the eventual outcomes. Just as with good software engineering in general, we start by clarifying the program scope. Let’s ask ourselves two questions:</p>
<ol>
<li>What does the program <em>need</em> to do?</li>
<li>What does <em>low latency</em> mean in the context of this program?</li>
</ol>
<p>Take a few minutes to ponder the two questions before I unpack their relevance in the following paragraphs. This is a good learning practice in general as it helps prepare your mind for ingesting new information.</p>
<h2 id="functional-requirements">Functional Requirements</h2>
<p>Let’s think about the first question: What does the program <em>need</em> to do?</p>
<p>Firstly, I want to go over a semantic detail. I use the word program above, but the question applies to any level of abstraction; from high-level to low-level, from distributed system to a single function. Apply the inquiry to whatever you are trying to build or optimize for low latency.</p>
<p>Secondly, notice the emphasis on the word ‘need’. You are not asking yourself about wants or nice-to-haves. You are strictly thinking about what the functional unit cannot exist without. While this sounds like an obvious question, it actually helps us transition to a different mindset than latency-agnostic programming. ‘Everything’ that is not necessary should be potentially be removed. Let me list some examples to illuminate what I mean. Don’t use a data structure that maintains a sorted order if it is not necessary. Don’t log unless it is necessary. Don’t make a copy of data unless it is necessary. Don’t publish the data to an archive unless it is necessary. Don’t use a library that does more than you need. You <em>must</em> adopt a ruthless mindset when it comes to ripping out functionality and execution steps. Everything must justify its existence in your systems and code.</p>
<p>Sometimes, asking this question ‘hard enough’ will reach back and influence your functional business requirements. If the operations necessary for enabling a feature are deemed ‘too slow’ in the best case, perhaps the business requirements are flawed and must be changed. For most domains, there is a two-way relationship between low-latency abilities and business requirements, as lower latency can potentially open up new business opportunities/functionality, and business requirements can sometimes help push for new latency standards that were once thought impossible. There is a delicate balance to be found and a healthy relationship between the two is important for overall success.</p>
<h2 id="defining-low-latency-metrics">Defining Low Latency Metrics</h2>
<p>And now let’s shift our attention to the second question: What does <em>low latency</em> mean in the context of this program?</p>
<p>Answering this question will help us define metrics and goals. When defining your metrics you must identify relevant operations and their respective start and end points. For example, if you are writing a web server this could be the ‘request to response’ duration. Or if you are optimizing a single function, it is the ‘call to return’ duration.</p>
<p>As for defining goals, there are two different options. Either you define a strict, absolute time duration requirement for the relevant operations or simply give yourself a mandate to ‘be as fast as possible’. The former has a strict cut-off, duration wise, as to what is acceptable. The latter simply means you are aiming to reduce the duration between start to end time points as much as you can. While being ‘as fast as possible’ is always a good goal to have, the specific acceptable durations can be crucial from a business perspective and can help provide the push necessary to truly focus on lower latency.</p>
<p>Preparing the metrics and their related goals are incredibly important.</p>
<p>If the goal is a strict absolute time duration requirement it will give us an opportunity to analyze and determine whether it is even possible to fulfill; preventing wasted development time. For example: a business requirement requires <em>parsing and processing</em> a large incoming JSON message in 50 microseconds. You know that the absolute fastest parsing of a JSON message of that size on your target host/architecture takes 55 microseconds. Taking these facts into consideration, you will know it is simply an impossible ask.</p>
<p>Additionally, it helps us to define measurable objectives for project success. This is important in any endeavor, programming or otherwise. If you don’t have a way to measure your deliverables, you simply won’t know if you are succeeded or failing. Having these measurable objectives will also be useful during development time to track your progress and inform you if your program design is working.</p>
<h2 id="chapter-summary">Chapter Summary</h2>
<ul>
<li>Clarify your functional requirements. Be extra aggressive when throwing out unnecessary features.</li>
<li>Understand the two-way relationship between low latency objectives and business requirements.</li>
<li>Define your low latency metrics. Use these throughout development and completion to measure project success.</li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Why I Still Find Programming Inspiring, 8 Years Later</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/why-i-still-find-programming-inspiring-7-years-later/"/>
        <id>https://tech.davidgorski.ca/why-i-still-find-programming-inspiring-7-years-later/</id>

        <updated>2024-03-13T14:46:39-04:00</updated>
            <summary>
                <![CDATA[
                    I’ve been programming for 8 years now. While there have been ups&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>I’ve been programming for 8 years now. While there have been ups and downs, I’ve recently had a chance to reflect on my dual choice in career and hobby. It still means a lot to me.</p>
<p>As an act of creative expression, it is truly unique. Programming is the act of creating <em>moving poetry</em>. Programs are not merely words at rest, they define and prescribe actions that will take place and interact with each other. As a form of writing it is also much more dynamic; as code is more likely to be updated than articles, books or blogs.</p>
<p>It also includes a foil to the creativity. There is a requirement for the application of the muse to be logically consistency. Code isn’t just free-for-all modern art. You have real rules to play within; providing a grounded world to inhabit. This ‘grounding’ provides objectivity to the sport and allows comparison and competition between pieces.</p>
<p>Also, programming is hilariously useful. It enables our modern world to run. To re-quote for the millionth time: <em>Software is eating the world.</em> And it is not for nothing: automation frees humanity of mundane, repeat tasks while unlocking a new form of leverage. Write once; run infinity times. This is especially mind-boggling when realizing the reduced capital requirements in the face of traditional industries.</p>
<p>It can be easy to become jaded towards one career and one’s craft. The fun, art, hacking is often overshadowed by business requirements and politics. I get that. But, when you pause and think, it is truly magical we get to make money with intellectual self-reflection.</p>
<p>I realized that this is precisely the dark side of it all; the self is a little too involved in the job. It touches the ego, the intellect and the creative identity. For such a well-compensated and flexible job, the number of depressed and burnt-out developers is quite high.</p>
<p>It has been prognosticated many times that the end of programming is nigh; thanks to the development of new no-code tools. And maybe this time, with AI and large language models, that will be true. But that doesn’t change that for a brief moment in history, humans could talk to machines using their native language. And that’s more than kinda neat.</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Introduction To Low Latency Programming: External Processing</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/introduction-to-low-latency-programming-external-processing/"/>
        <id>https://tech.davidgorski.ca/introduction-to-low-latency-programming-external-processing/</id>
            <category term="Low Latency"/>
            <category term="C++"/>

        <updated>2024-03-11T11:39:16-04:00</updated>
            <summary>
                <![CDATA[
                    This post originally appears as a chapter in my new book: ‘Introduction&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href="https://a.co/d/0U6KOfb">Available now for purchase on Amazon</a>.</em></p>
<p>After heavily scrutinizing your program scope you should be left with some functional requirements that are absolutely mandatory, as well as some latency goals you are aiming for. From here, you can start thinking about which pieces of the program could potentially be extracted from the main execution path, to keep it as fast as possible. For our purposes, the <em>main execution path or task</em> is process, system or function that is measured by our defined metrics. Here are some questions you can ask yourself to prepare for this process:</p>
<ul>
<li>What is known before the program even runs?</li>
<li>Can we use this information to ‘do’ some steps before runtime?</li>
<li>Does any part of the runtime process need to be done ‘real time’?</li>
<li>Can any of the runtime steps be isolated into a task that can be packaged into a cohesive unit?</li>
<li>What is the potential cost of communicating with the main task?</li>
</ul>
<h2 id="pre-processing">Pre-processing</h2>
<p>When we are discussing ‘low latency’ we are almost always referring to latency at runtime. Therefore, if we can remove segments of the program functionality and execute them before we enter our low latency execution; we can effectively get them done for ‘free’.</p>
<p>If you have a lot of information that is used as an input to the runtime computation available to you before the process is even started, you can attempt to perform some parts of the computation even before the program is run. The following subsections discuss some common approaches.</p>
<h3 id="configurationinput-files--loading-on-start-up">Configuration/Input Files + Loading On Start Up</h3>
<p>One approach is to generate configuration/input files and supply them to the process at startup. This is very simple from an operation and code perspective. You can create a separate process, or group of processes, that reads from any number of inputs, performs necessary computations and then outputs the results into a file. This file is then read by the low-latency process at start up. Steps:</p>
<ol>
<li>Run external process that completes and generates a file.</li>
<li>Start the low-latency task/runtime, supplying the file as input.</li>
</ol>
<p>Additionally, if you don’t need the preparation functionality to be uncoupled from the process, you can have the main process itself do this computation before it enters the low-latency execution phase. This is simple and has an even lower operational overhead as the preparation code lives in the same code ‘space’ as the low-latency code. Warming process steps:</p>
<ol>
<li>Warming up: read various database tables into memory, calculate some weights, etc.</li>
<li>Enter low-latency runtime loop.</li>
</ol>
<h3 id="compile-time-processing">Compile Time Processing</h3>
<p>Another option is ‘hard-coding’ computation into the executable with compile-time programming options: code generation, templates, and ‘constexpr’ functions, methods and variables. These approaches can help reduce instruction count/cost and branching; both key goals of low latency programming (discussed later in the book). While they can provide ‘the fastest’ runtime, they may also be difficult to use in comparison to runtime options. However, they do allow us to squeeze out maximum performance and are often worth pursuing. They are ‘external’ in the sense that they don’t occur during our low-latency runtime operations, because they don’t occur at all. The removal decision is in some sense a type of processing.</p>
<h4 id="code-generation">Code Generation</h4>
<p>Code generation takes on many forms. It is the creation of programs that generate code as output. The inputs provided to the code generation program will determine the end functionality included in the final code. This allows us remove ‘work’ and decision-making out of the final runtime and perform these ‘steps’ before or at compile time. The code generator does not have to meet your low latency goals itself, as it is the final runtime process performance that actually matters. Here is a sample code generation script execution:</p>
<pre><code class="language-bash">python3 generateCode.py --input inputs.json --output output.cpp
</code></pre>
<p>The <strong>inputs.json</strong> file will dictate what will be included in the final code within the <strong>output.cpp</strong> file.</p>
<h4 id="compile-flags-and-templates">Compile Flags and Templates</h4>
<p>You can build programs that use C++ template arguments for specialization: both for types and values. Then, at compile time you can supply flags that will select the correct specialization. Everything unrelated to your supplied options will not be included in the runtime. The following is a super simple example:</p>
<pre><code class="language-c++">#ifdef BUYER
  using OrderManager = BuyOrderManager;
#else
  using OrderManager = SellOrderManager;
#endif
System&lt;OrderManager&gt; orderManager;
</code></pre>
<p>The <code>System</code> class takes <code>OrderManager</code> as a template argument and passes it down to other child members. The value of this template argument changes its runtime behavior at compile time; effectively doing the ‘decision-making’ before the program is executed. Of course, we need to pass in the <code>BUYER</code> flag during compilation:</p>
<pre><code class="language-bash">g++ -o trade_app trade_app.cpp -DBUYER
</code></pre>
<h4 id="leveraging-constexpr-utilities">Leveraging ‘constexpr’ Utilities</h4>
<p>Modern C++ (since C++11) has given us <code>constexpr</code> which allows marking variables, functions, classes and methods with a hint that expressions can be resolved at compile time. Here are the ‘rules’:</p>
<ul>
<li>A constexpr variable must be initialized as a constexpr type with a constepxr function. These must be constructed/called with either literals or other constexpr variables as arguments.</li>
<li>A constexpr function can only take and return constexpr types. The key thing to know is that arguments can be accepted as constexpr, but not passed on as constexpr. Once within the function you cannot guarantee the argument values are known at compile time, even though they might be.</li>
<li>A constexpr type/class can only have constexpr type members and must at least one constexpr constructor. All scalar types and arrays are considered constexpr.</li>
</ul>
<p>Even with this constrained set of functionality, we can do some pretty fancy stuff. All the way from specializing function bodies using template parameters to parsing JSON at compile time. The rabbit hole goes deep. In a way it is a form of ‘code generation’ included in C++. Other languages have other forms of compile time pre-processing.</p>
<p>Using <code>constexpr if</code> and a template argument to specialize function body:</p>
<pre><code class="language-c++">
template&lt;bool DoOne&gt;
void process() {
  if constexpr (DoOne) {
    // ...
  } else {
    // ...
  }
}
</code></pre>
<p>Calculating hash at compile time:</p>
<pre><code class="language-c++">template&lt;size_t Length&gt;
constexpr size_t hash(const char(&amp;str)[Length]) {
  size_t result = 0;
  for (size_t i = 0; i &lt; (Length - 1); i++) {
    result ^= str[i] &lt;&lt; (i % 8);
  }
  return result;
}

// ...
constexpr auto hashValue = hash(&quot;Hello, World!&quot;);
</code></pre>
<p>Every subsequent version of C++ has significantly boosted the functionality provided by constexpr. Make as much of your code and static data ‘constexpr’ as possible. I have to note something here; The benefit of constexpr is not just the reduction of instructions. You could do that with your own precalculation and storage of static data in an executable written in assembly. This, however, would be incredibly tedious. <code>constexpr</code> is powerful because it treats compile-time resolved data just as it does code; something that can be understood by the programmer and to be updated as objectives/requirements evolve. That is the true power of the constexpr ‘contract’.</p>
<h2 id="external-parallel-processing">External Parallel Processing</h2>
<p>Sometimes information is not known before runtime; it requires input from data received during runtime or is updated throughout the day from external sources or computation. This section will discuss a few ways that external processing can be done when the program is already running.</p>
<h3 id="separate-process">Separate Process</h3>
<p>Our first option is to have external processes that perform the extracted tasks. This is usually the case for data that does not require inputs from the process itself. Perhaps this is the result of some grid computing batch result or incoming network data. In any case, after the external process is done, it needs a way to pass this data into the already-running main process. The selection of communication method is dependent on data size and the latency requirement. If this is a change that occurs a few times a day we’d use one approach as opposed to a message that is sent every 10 microseconds.</p>
<p>If this communication rarely happens we can choose a simple method with a low operational overhead such as writing the results to a file and signalling the already running process to read the file. This would be useful for something like a configuration file change or perhaps loading a new machine learning model weights. <strong>SIGUSR1</strong> and <strong>SIGUSR2</strong> are the signals reserved for the program’s own use. The safest way to do this is to register simple flag-setting handler functions at startup which will be called when the signal is received. Handler complexity itself should be kept to a minimum:</p>
<pre><code class="language-c++">#include &lt;signal.h&gt;
#include &lt;iostream&gt;
#include &lt;thread&gt;

// Global variable to indicate if a reload is needed
bool performReload = false;

// Signal handler
void handler(int) {
  performReload = true;
}

int main() {
  // Register signal handler
  signal(SIGUSR1, handler);

  // Enter main loop
  while(true) {
    if (performReload) {
      std::cout &lt;&lt; &quot;Reloading configuration...&quot; &lt;&lt; std::endl;
      performReload = false;
    }
    // Actual work
    std::this_thread::sleep_for(std::chrono::seconds(1));
  }
  return 0;
}
</code></pre>
<p>However, if this communication stream is almost constant, we would have to select a different option. For low latency programs, the common choice is using shared memory with atomic access operations. This leverages operating system constructs that allow two different processes to access the same block of memory; something forbidden by default. Atomic operations are instructions which guarantee atomic accesses and updates of memory. In simpler terms; you are able to assume that no other thread/process is able to read/write to the variable while you are reading or writing to it. The other accessor will either get the old version or the new version depending on instruction scheduling and/or atomic sequencing level chosen. Without using atomic operations you might read memory with an in-between state. There are multiple levels of guarantee:</p>
<ul>
<li>Relaxed</li>
<li>Consume</li>
<li>Release</li>
<li>Acquire</li>
<li>Acquire/Release</li>
<li>Sequentially Consistent</li>
</ul>
<p>You can read more about them by reading the documentation of <code>std::atomic::memory_order</code>. In general, as you move down the list, the guarantees get stronger and stronger about how memory is to be written/read as other threads/processes do the same. Really think about how your memory will be accessed and what the reader/writer really needs to be guaranteed in order to fulfill their objectives. Keep in mind: stronger guarantees cost more; time-wise.</p>
<p>Once you have two processes accessing the same memory space, you can design data structures that enable communication between the two. The favorite choice for low latency applications is a ring buffer implemented as a contiguous array in memory. The API/design differs based on different access scenarios; mainly affected by the number of simultaneous consumers or producers. A single producer/consumer queue is very simple to implement. The following example is not optimized, but captures the general idea. Try to walk through how the reader and writer each would access the memory with atomic and non-atomic instructions. Think about how the access indices interact and why they begin with an offset from one another. After reading about <code>std::atomic::memory_order</code>, which guarantees do we need for which operations? By default, they use <code>memory_order_seq_cst</code>; with the strictest guarantees.</p>
<p>A simple single producer, single consumer wait-free queue:</p>
<pre><code class="language-c++">template&lt;typename T, size_t N&gt;
class WaitFreeQueue {
  T _data[N];
  std::atomic&lt;size_t&gt; _readSequence = 0;
  std::atomic&lt;size_t&gt; _writeSequence = 1;
public:
  WaitFreeQueue() = default;

  bool tryWrite(T value) {
    const auto nextWriteIndex = _writeSequence % N;
    const auto currentReadIndex = _readSequence % N;
    const bool noRoomLeft = (nextWriteIndex == currentReadIndex);
    if (noRoomLeft) {
      return false;
    }
    _data[nextWriteIndex] = std::move(value);
    _writeSequence.store(nextWriteIndex + 1);
    return true;
  }

  T* tryRead() {
    const auto nextReadIndex = (_readSequence + 1) % N;
    const auto nextWriteIndex = _writeSequence % N;
    const bool noNewData = (nextReadIndex == nextWriteIndex);
    if (noNewData) {
      return nullptr;
    }
    _readSequence.store(nextReadIndex);
    return &amp;_data[nextReadIndex];
  }
};
</code></pre>
<p>To use this across processes, you would create a shared memory segment and construct this queue in the memory space in <strong>one</strong> process using placement new: <code>new (shared_memory_ptr) WaitFreeQueue&lt;int, 100&gt;();</code>. The other processes would simply interpret the shared memory pointer as the queue and use it.</p>
<p>If you try to extract the address pointer out of one process and use the raw value in another; your second process would be crashed by the operating system. It would be trying access memory outside its virtual memory space and this is forbidden.</p>
<h3 id="separate-thread">Separate Thread</h3>
<p>Sometimes this external computation requires data that the main process provides during runtime. In that case it might be more ergonomic to perform the computation in the same process but on a different thread. For the most part the performance distinction between processes and threads can be blurry. For many operating systems, they operate in the same way once you attach to the same shared memory region. The decision would be more so about the level of coupling you want between the foreground and background task runners. In this section we will be discussing a background task that is: 1. Critical to the runtime of the foreground task. 2. In constant back-and-forth communication with the foreground task. Therefore, it is probably better to use a thread for this job rather than a separate process. This can be summarized into the following:</p>
<ul>
<li>Use a process if you want the background task to run uncoupled with the foreground task. I.e. you are okay with each task to run separately.</li>
<li>Use a thread if you want the background task to be coupled with the foreground task. I.e. you need both tasks to be running constantly.</li>
</ul>
<p>One thing to note that is applicable to both separate thread and separate process asynchronous tasks: <strong>The communication mechanism overhead must be less costly to the foreground task then just performing the task itself.</strong> Otherwise, what is the point of doing it in the background?</p>
<p>Just as with separate processes, using a separate thread to perform tasks will ideally be coordinated using atomic variables as they can allow for ‘wait-free’ operations. You will not be waiting for a non-deterministic amount of time/CPU clock cycles. The same data structures/instructions you use with a separate process can be used across threads.</p>
<h2 id="chapter-summary">Chapter Summary</h2>
<ul>
<li>You can remove expensive computation outside your program to help reduce latency.</li>
<li>These computations can be done at compile time, at start-up, or in an external thread or process.</li>
<li>If the processing is done externally, ensure you select the best communication option for the job.</li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Introduction To Low Latency Programming: Understand Storage</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/introduction-to-low-latency-programming-understand-storage/"/>
        <id>https://tech.davidgorski.ca/introduction-to-low-latency-programming-understand-storage/</id>
            <category term="Low Latency"/>
            <category term="C++"/>

        <updated>2024-03-04T12:01:37-05:00</updated>
            <summary>
                <![CDATA[
                    This post originally appears as a chapter in my new book: ‘Introduction&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href="https://a.co/d/0U6KOfb">Available now for purchase on Amazon</a>.</em></p>
<p>So far we have focused on the computation aspect of low latency programming. However, optimized data access plays a huge role in this domain. No process is computation only, it needs data to compute on. This chapter will discuss the key ideas to keep in mind for the optimization of reading and writing from storage.</p>
<h2 id="be-aware-storage-performance-and-access-costs">Be Aware Storage Performance And Access Costs</h2>
<p>I am using the word ‘storage’ quite broadly in this chapter; referring to persistent storage, temporary storage, local storage and network storage. From our perspective, we mainly care about ‘time to access’, rather than durability, geographic location or any other ‘implementation’ details.</p>
<p>It is very important to know the rough time it takes to access different storage sinks. In general, this time increases exponentially as the physical distance from the actual CPU core increases. The following list is ordered from closest to furthest for a theoretical CPU core including the rough ‘time to access’ expressed as a multiplier:</p>
<ol>
<li>Registers: 1x</li>
<li>L1 cache: 5x</li>
<li>L2 cache: 10x</li>
<li>L3 cache: 50x </li>
<li>Same-NUMA memory: 100x</li>
<li>Different-NUMA memory: 300x</li>
<li>Persistent Storage (SSD/HDD/Tape): 10000x minimum</li>
<li>Local Data Center: 200000x</li>
<li>Distant Data Center: 100000000x</li>
<li>The Moon: 1000000000000x</li>
</ol>
<p>Our closest example storage type is the processor-local set of registers. These are the fastest accessible locations to the processor and are used as inputs and outputs for various computations. Most computers typically operate by loading data from the main memory into these registers, performing some instructions which save results to a register, and finally writing the values from registers into the main memory. Our code is turned into instructions which work with registers. The various levels of cache are optimizations for accessing the main memory.</p>
<p>The furthest example storage type is a theoretical network drive that is within a computer on the moon. If a program wants to read or write to it, it will have to issue the command over a network spanning 384,400 kilometers. It is obvious that until we develop spacetime wormholes, greater physical distance translates into higher communication latency.</p>
<p>Keep this list in mind when designing your programs. For low-latency applications, you will ideally never go beyond ‘same-NUMA memory’ after initial start-up is complete. <em>NUMA (Non-Uniform Memory Architecture)</em> is a multiprocessing architecture feature that assigns memory to specific CPU cores in an effort improve overall memory performance. The key takeaway: <em>try to keep your data as close the CPU as possible</em>. Of course, if our program has data persistence requirements, you will factor that in to your design.</p>
<h2 id="caching">Caching</h2>
<p>With most modern architectures, continuous memory is loaded together as a group. When a memory address is requested to be read, surrounding data will be pulled into the various cache layers as well. This group of data is called a ‘cache line’. You can see the size of the cache line on your platform with the <strong>std::hardware_destructive_interference_size</strong> value in C++.</p>
<p>Keeping cache mechanics in mind is important as you design your algorithms and data structures. Factoring them into your program implementation can greatly increase performance and therefore reduce latency. In general, to keep a portion of code running as fast as possible, have it access as small a range of continuous memory as possible.</p>
<p>Put data that will be accessed as ‘a unit’ as close as possible; ideally within one cache-line for maximum performance. There are still huge benefits for using continuous memory beyond that. For example, adjacent cache lines are often prefetched. Generally use data structures that are as flat as possible (in contiguous memory) like static or dynamic arrays. They should be the default data structure of choice. Additionally, design your algorithms to fully process a block of contiguous memory before moving on (avoid going back). Sometimes this means re-arranging the order of some loops, such as with this simple matrix multiplication function:</p>
<p>Before:</p>
<pre><code class="language-c++">using Vector = std::vector&lt;float&gt;;
using TwoDimMatrix = std::vector&lt;Vector&gt;;

TwoDimMatrix matMul(const TwoDimMatrix&amp; m1, const TwoDimMatrix&amp; m2)
{
  const auto sharedDim = m1.front().size();
  TwoDimMatrix result(m1.size(), Vector(m2.front().size()));
  // Iterate over rows of m1
  for (size_t i = 0; i &lt; m1.size(); ++i) {
    // Iterate over rows of m2
    for (size_t j = 0; j &lt; m2.size(); ++j) {
      // Iterate over cells of m1 row and m2 column
      for (size_t k = 0; k &lt; sharedDim; ++k) {
        result[i][j] += m1[i][k] * m2[k][j];
      }
    }
  }
  return result;
}
</code></pre>
<p>After:</p>
<pre><code class="language-c++">TwoDimMatrix matMul(const TwoDimMatrix&amp; m1, const TwoDimMatrix&amp; m2)
{
  const auto sharedDim = m1.front().size();
  TwoDimMatrix result(m1.size(), Vector(m2.front().size()));
  // Iterate over rows of m1
  for (size_t i = 0; i &lt; m1.size(); ++i) {
    // Iterate over cells of the m1 row and rows of m2
    for (size_t k = 0; k &lt; sharedDim; ++k) {
      // Iterate over cells of the m2 row
      for (size_t j = 0; j &lt; m2.size(); ++j) {
        result[i][j] += m1[i][k] * m2[k][j];
      }
    }
  }
  return result;
}
</code></pre>
<p>All we really did was switch the k and j loop nesting order. This change can yield up to a 10x speed up as iterating over a column is much more expensive than iterating through a row. Why? The row is continuous in memory, whereas the column is not. Every time we access the next value in the column we are actually accessing a relatively ‘distant’ part of memory.</p>
<p>Another thing to keep in mind is ‘false sharing’. If you are using atomic instructions on two memory locations that are within a single cache line, you could inadvertently slow down your program as the CPU will send an instruction to lock the cache line from other cores. This is unavoidable and desired if the two cores are actually accessing the same memory location, but if it just happens to be nearby and not the <em>same exact</em> memory location, we are paying a price for nothing. To mitigate this, you want to make sure the two memory addresses are on separate cache lines using the C++ <code>alignas</code> keyword:</p>
<pre><code class="language-c++">struct DoNotShare {
  alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; one;
  alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; two;
};
</code></pre>
<p>This will prevent any false sharing from occurring by aligning the variables to offsets defined by the integer value returned by <strong>std::hardware_destructive_interference_size</strong>. Keep this in mind when using atomic instructions on nearby memory addresses.</p>
<h2 id="chapter-summary">Chapter Summary</h2>
<ul>
<li>Know the rough time taken to access different storage sinks. From same-NUMA memory cache line all the way to a computer sending RF signals from the moon.</li>
<li>Understand the cache characteristics of your hardware.</li>
<li>Keep in mind that that measurement is key; educated assumptions can be useful early on in the design process, but to derive conclusions you must measure performance yourself.</li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Introduction To Low Latency Programming: Minimize Branching And Jumping</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/introduction-to-low-latency-programming-minimize-branching-and-jumping/"/>
        <id>https://tech.davidgorski.ca/introduction-to-low-latency-programming-minimize-branching-and-jumping/</id>
            <category term="Low Latency"/>
            <category term="C++"/>

        <updated>2024-02-26T15:42:03-05:00</updated>
            <summary>
                <![CDATA[
                    This post originally appears as a chapter in my new book: ‘Introduction&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href="https://a.co/d/0U6KOfb">Available now for purchase on Amazon</a>.</em></p>
<p>This chapter will discuss how branching and jumping in our code affects our runtime performance and how we can avoid them in our effort to reduce the latency of our programs. <strong>Branching</strong> refers to process execution that can go down one of multiple paths. This functionality is provided by ‘check and jump’ instructions. <strong>Jumping</strong> refers to when the program control pointer is changed to a different location in memory rather than progressing to the next sequential instruction. This can be conditional (as a part of branching) or unconditional.</p>
<h2 id="what-does-it-cost">What Does It Cost?</h2>
<p>Why do we want to discourage Jumping and/or Branching? Firstly, ‘comparing and jumping’ simply adds more instructions. As mentioned throughout the book, we want to minimize instructions overall. Secondly, branching disrupts the smooth flow of instruction prefetching, meaning if a condition that was not guessed by the branch predictor is encountered, the instruction pipeline has to potentially be flushed and refreshed. A new sequence of instructions will have to be retrieved.</p>
<p>The branch predictor is an element of CPUs that aims to predict which branch will be taken in our code during runtime. Branch predictors have gotten more and more complex, but the fundamental idea is maintaining branching statistics and using them to determine where a program control address branch will most likely jump to. As mentioned in the previous paragraph, that prediction allows the pre-fetch of upcoming instructions.</p>
<p>More instructions and instruction pipeline invalidation both contribute to the stalling of program execution. And stalling means we are not running the important instructions that directly contribute to fulfilling the business objectives. In an ideal world, our program would run these ‘golden instructions’ uninhibited.</p>
<h2 id="examples-of-branching-and-jumping">Examples Of Branching And Jumping</h2>
<p>The following short list contains <em>some</em> common code constructs which will generate jumping and/or branching instruction constructs:</p>
<ul>
<li><strong>Branching and Jumping</strong>: If statements, Chained boolean conditionals, Virtual method calls</li>
<li><strong>Jumping:</strong> Function calls</li>
</ul>
<h2 id="steps-to-reduce-branchingjumping">Steps To Reduce Branching/Jumping</h2>
<h3 id="branchless-boolean-expressions">Branchless Boolean Expressions</h3>
<p>Standard boolean expression operations such as <code>&amp;&amp;</code> and <code>||</code> have ‘short-circuiting’ as a feature; if previous boolean expressions evaluate to false, consecutive expressions will not be evaluated:</p>
<pre><code class="language-c++">const bool andExpression =
  false /*will be evaluated*/ &amp;&amp;
  true /* will not be evaluated*/;
const bool orExpression =
  true /*will be evaluated*/ ||
  false /* will not be evaluated*/;
</code></pre>
<p>As expected, this introduces branching to our code and should be considered for removal. What do we use instead? You can use the bitwise AND and OR expressions:</p>
<pre><code class="language-c++">const bool andExpression = false &amp; true; // Both will be evaluated
const bool orExpression = true | false; // Both will be evaluated
</code></pre>
<p>When should you switch to this approach? There is a simple rule of thumb: switch from logical to bitwise boolean expressions when the overhead cost of branching does not outweigh the cost of evaluating all terms in the expression.</p>
<p>It still makes sense to use logical boolean expressions if consecutive expressions are expensive to evaluate. Such the following:</p>
<pre><code class="language-c++">const bool result = cheapCheck() &amp;&amp; expensiveCheck();
</code></pre>
<p>The short-circuiting branch allows us to avoid the expensive check, which the branchless, bitwise version would not. Tip: For the logical flavor, it makes sense to arrange the expressions in a chain from least to most expensive to enable short-circuiting before the largest computation costs would be incurred.</p>
<p>Take a good look at the cost of the expression terms before deciding on using the branching or branchless boolean expressions.</p>
<h3 id="write-code-which-generates-cmov-like-instructions">Write Code Which Generates ‘CMOV’-like instructions</h3>
<p><code>CMOV</code> is a built-in instruction on the x86 architecture which does a ‘conditional move (copy)’. While spiritually similar to a branch, it is significantly cheaper than real ‘check and jump’. Since we are not writing assembly by hand, we won’t be directly implementing <code>CMOV</code> use. Rather, we will try to use code constructs that will be turned into <code>CMOV</code> instructions. One of these is ternary operator. Regardless, a good compiler will use <code>CMOV</code> when it can, even for if statements (when they simply assign to a value to a variable).</p>
<p>Ternary operator use:</p>
<pre><code class="language-c++">const int output = useFortyTwo() ? 42 : 24;
</code></pre>
<p>Make sure to measure the actual effect of using <code>CMOV</code> instructions with your program and data. Sometimes well-predicted branches can be more performant than <code>CMOV</code>.</p>
<h3 id="remove-the-use-of-virtual-functions">Remove The Use Of Virtual Functions</h3>
<p>Virtual function calls perform a jump. When it comes time to call the virtual function, the generated virtual lookup table is consulted for that class. Based on the values in the table, the correct implementation of the method will be ‘jumped to’ to in the executable. This incurs a cost to load the vtable as well a branching jump. Ideally, we will avoid virtual function calls in our programs. If you are using purely virtual classes as a form of interface, you can consider using C++20 concepts instead. They will allow you to constrain generic code and allow you to create ‘interfaces’ via <code>requires</code>.</p>
<p>Of course, for leveraging dynamic dispatch (runtime specified code execution) there is no alternative to runtime branching / jumping. Other approaches such as using sum types (<code>std::variant</code>) or type erasure all perform the same relative types of instruction. Think about if you really need dynamic dispatch.</p>
<h3 id="inline-your-functions">Inline Your Functions</h3>
<p>Function inlining is another feature we can leverage to reduce jumps. In addition to jumping to a different location in our code and dealing with the costs of that, we also don’t have to pay the function call overhead. This overhead includes saving the register states, filling registers and/or the stack with the function arguments, incrementing the stack pointer, saving the stack return location and saving the current program control location. Function inlining puts a copy of the function body being called right at the call site. This can result in some great performance gains as there is no jump and the instruction pipeline is as simple as possible and spatially localized. You can use the GCC <code>always_inline</code> attribute above functions you want to be strictly inlined. For example:</p>
<pre><code class="language-c++">[[gnu::always_inline]
int function(int a) {
  return a + 11232;
}
</code></pre>
<p>Even though this function would have most likely already been inlined due to its simplicity, this attribute will forcefully encourage the compiler to do so, provide a warning to us if it is unable to, and also encodes the inlining objective within the code itself for all programmers to see.</p>
<h2 id="compiler-hints">Compiler Hints</h2>
<p>If branching is absolutely necessary in your code, you can use compiler hints to provide the compiler with extra information that it cannot infer. Using this information, the compiler will potentially re-order instructions in a way that will make the branch you marked as more ‘likely’ to be more efficient from a conditional branching perspective. This can be done using the <code>__builtin_expect</code> function or from C++20 and above with the <code>likely</code> and <code>unlikely</code> attributes. You can make the <code>__builtin_expect</code> function more ergonomic to use by creating <code>LIKELY</code> and <code>UNLIKELY</code> macros:</p>
<pre><code class="language-c++">#define LIKELY(x) __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
</code></pre>
<p>Let’s explore a simple use case and it’s effects. Take a look at this simple function, and it’s corresponding assembly:</p>
<p>C++:</p>
<pre><code class="language-c++">void func(int input) {
  if (input == 1) {
    // BRANCH ONE
  } else if (input == 2 ) {
    // BRANCH TWO
  } else {
    // BRANCH THREE
  }
}
</code></pre>
<p>Assembly:</p>
<pre><code class="language-asm">func(int): # @func(int)
  movq %rdi, %rax
  leaq 1(%rdi), %rcx
  cmpl $2, %esi
  je .LBB0_3
  cmpl $1, %esi
  jne .LBB0_4
  # BRANCH ONE
  retq
.LBB0_3:
  # BRANCH TWO
  retq
.LBB0_4:
  # BRANCH THREE
  retq
</code></pre>
<p>Notice that the <code>cmpl</code> instruction is first executed with ‘2’ as the argument. Only after, if we didn’t jump, it will execute with ‘1’. What if we know that the value in <code>$esi</code> is more likely to be equal to ‘1’ and we want to prioritize that branch? Let’s add a <code>LIKELY</code> annotation to our C++ code:</p>
<pre><code class="language-c++">void func(int input) {
  if (LIKELY(input == 1)) {
    // BRANCH ONE
  } else if (input == 2 ) {
    // BRANCH TWO
  } else {
    // BRANCH THREE
  }
}
</code></pre>
<p>Now we can observe the different in output assembly:</p>
<pre><code class="language-asm">func(int): # @func(int)
  pushq %rbx
  cmpl $1, %esi
  jne .LBB0_1
  # BRANCH ONE
  retq
.LBB0_1:
  cmpl $2, %esi
  jne .LBB0_4
  # BRANCH TWO
  retq
.LBB0_4:
  # BRANCH THREE
  retq
</code></pre>
<p>As you can see, the compiler re-arranged the branching instructions. The comparison of <code>%eri</code> with 1 is now part of the first call to <code>cmpl</code> + <code>jne</code> and if equality is determined we don’t jump anywhere else. Spatially, we are already at the <code>BRANCH ONE</code> code. The compiler also moved the remaining conditionals after the first jump at <code>.LBB0_1:</code>. It is quite apparent that all remaining cases are deprioritized from an optimization standpoint in comparison to <code>BRANCH ONE</code>.</p>
<p>Now for some semantic rambling. Even though they have been accepted as standard terms, likely and unlikely are not really accurate from a definition perspective. You are not really marking that one conditional branch is more likely than the other to occur. What you are doing is telling the compiler that you want to optimize for this conditional branch. Perhaps <code>OPTIMIZE</code> and <code>UNOPTIMIZE</code> are more accurate terms for our construct. Making your code as readable/understandable/surface-level as possible is always something worth pursuing.</p>
<h2 id="cheaper-branching">Cheaper Branching</h2>
<p>Not all ‘branching’ instructions are created equally. Consider the following function that that performs a few <strong>compare and jumps</strong>:</p>
<pre><code class="language-c++">void processThisString(std::string_view input)
{
  if (input == &quot;production&quot;) {
    processProd(input);
  } else if (input == &quot;RC&quot;) {
    processRC(input);
  } else if (input == &quot;beta&quot;)
    processBeta(input);
  }
}
</code></pre>
<p>This is a ‘run-of-the-mill’ if statement. However, we have an opportunity here: the comparison set is very constrained. In fact, most the calls to the <code>==</code> operator distill to a <code>memcmp</code> which will short circuit after the first character, since all the first characters in the string literals are different. For example, if the input’s value is “beta”, we will perform two calls to <code>==</code> for nothing. Instead, we can re-write this in a more efficient manner by using the <code>switch</code> construct on that first character:</p>
<pre><code class="language-c++">void processThisString(std::string_view input)
{
  constexpr auto i = 0;
  switch (input[i]) {
    case &quot;production&quot;[i]: processProd(input); break;
    case &quot;RC&quot;[i]: processRC(input); break;
    case &quot;beta&quot;[i]: processBeta(input); break;
  }
}
</code></pre>
<p>The compiler will most likely generate a more efficient branching construct using jump tables and binary decision trees since data of only 1 char width is being compared now. A best-case scenario would be if the range of first character values is very limited, because then a single jump table would be used to increment the program control pointer. Of course, this is not cut and dry, because a small set of inputs may cause an if statement to perform better. Always measure! Regardless, this is a good approach to keep in mind for optimization.</p>
<h2 id="chapter-summary">Chapter Summary</h2>
<ul>
<li>Jumping to distant code locations or branching can incur a heavy cost in low latency segments of the code. This is due to added instructions, instruction pipeline de-optimization and cache eviction.</li>
<li>Examples of Branching/Jumping include: if statements, chained conditionals, virtual method calls, and function calls in general.</li>
<li>You can avoid branching/jumping by using branchless conditionals, Using ‘CMOV’ instructions, Remove the use of virtual functions and try to inline your functions.</li>
<li>If branching is unavoidable, try to use a ‘cheaper’ variation of it.</li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Announcing My First Book: Introduction To Low Latency Programming</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/announcing-my-book-introduction-to-low-latency-programming/"/>
        <id>https://tech.davidgorski.ca/announcing-my-book-introduction-to-low-latency-programming/</id>

        <updated>2024-02-23T11:13:38-05:00</updated>
            <summary>
                <![CDATA[
                    I’m excited to announce the availability of my first technical book: Introduction&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>I’m excited to announce the availability of my first technical book:</p>
<p><strong>Introduction To Low Latency Programming</strong> </p>
<p>Amazon Link: <a href="https://a.co/d/0U6KOfb">https://a.co/d/0U6KOfb</a></p>
<p>It is a short, beginner-friendly entry into the domain of high-performance, bare metal coding. Learning low latency programming can be intimidating and online resources are scattered. Since there are no introductory books on the subject, my hope is that I can help programmers ‘kick start’ their entry into the domain gently. While this the book is not ‘deep’, it touches on important technical topics and ideas, provides many code examples and discusses the effects of modern hardware on optimization problems.</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Literary Programming</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/literary-programming/"/>
        <id>https://tech.davidgorski.ca/literary-programming/</id>
            <category term="style"/>
            <category term="Programming"/>

        <updated>2024-02-09T16:44:51-05:00</updated>
            <summary>
                <![CDATA[
                    Code is written once, but read many times. Given this truth, it&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Code is written once, but read many times. Given this truth, it makes sense to prioritize read-ability over write-ability. Readable code contains good variable names, uses appropriate abstractions and is understood easily. Many have waved the flag of ‘Clean Code’ for quite a few decades now.</p>
<p>What has helped me write code that I consider clear and readable is something I call <em>Literary Programming</em>. Not to be confused with <a href="https://en.wikipedia.org/wiki/Literate_programming">Literate Programming</a>. I would change the name of my concept but I’ve grown rather fond of it. Apologies in advance.</p>
<p>What I call <em>Literary Programming</em> is the deliberate application of structured grammar and story-telling into the naming, abstracting and composition of code. It is the conscious act of letting your code capture a narrative with your choice of words. Not leaving intentions and actions in a state of vagueness; method names should reflect what is actually being done within and the meaning of the return value, intermediate variables are encouraged.</p>
<p>Don’t overanalyze what I am trying to say. This isn’t a very strict set of rules. It may just be synonymous with ‘Good Code’ to you. It is a simple idea meant to inspire a little more scrutiny in you when creating APIs, choosing names, composing expressions and designing the structure of your application.</p>
<p>Here are just some ways you can make your more <strong>literary</strong>:</p>
<ul>
<li>Use abstractions to ‘shorten’ portions of code. Simple example: putting argument and configuration parsing in a separate function to keep the rest of the <code>main</code> as clean as possible.</li>
<li>Put great care into class API design; as much detail as possible into method names. Prefer a name like <strong>processIncomingMessages()</strong> over <strong>process()</strong> or <strong>messages()</strong>. <strong>getOrCreateItem()</strong> over <strong>getItem()</strong> or <strong>createItem()</strong> for a method that will return an existing object if it already exists.</li>
<li>Value consistency. One method shouldn’t be named <strong>bool error()</strong> while it’s counterpart is named <strong>bool isValid()</strong>.</li>
<li>Use intermediate variables to compose expressions. These variables serve as logic checkpoints for the future reader.</li>
<li>Wrap complicated loop conditions into a boolean-returning functor. Take this call site as an example: <code>while (continueRunning(itemsLeft, maxItemsToProcess, error))</code>.</li>
<li>As mentioned before; good naming pays dividends.</li>
</ul>

            ]]>
        </content>
    </entry>
    <entry>
        <title>C++ Pattern: Deriving From std::variant</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/c-mini-pattern-deriving-from-stdvariant/"/>
        <id>https://tech.davidgorski.ca/c-mini-pattern-deriving-from-stdvariant/</id>

        <updated>2024-01-31T20:59:35-05:00</updated>
            <summary>
                <![CDATA[
                    I am a big fan of sum types for expressive programming. They&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>I am a big fan of sum types for expressive programming. They provide an elegant way to encode mutually exclusive data types in a single field. While not provided by the language itself, the C++ standard library offers us <code>std::variant</code>. Since there is no language-level pattern matching construct, interacting with variants can be less than ergonomic. One way to mitigate this is inheriting from <code>std::variant</code> and creating useful domain-specific access methods. This article discusses a few different ways of deriving from <code>std::variant</code> that might be useful and/or interesting.</p>
<h2 id="a-result-type">A Result Type</h2>
<p>To start off we’ll create a ~15 line class derived from <code>std::variant</code> that fulfills the basics of a result type (something like C++23 <code>std::expected</code> but available in C++17 and above). This is a nice way to encapsulate success and failures types in a united interface. Implementation:</p>
<pre><code class="language-c++">#include &lt;variant&gt;

template&lt;typename T, typename Error&gt;
class Result : public std::variant&lt;T, Error&gt; {
public:
  using std::variant&lt;T, Error&gt;::variant;
  using std::variant&lt;T, Error&gt;::operator=;
  
    Error* error() { return std::get_if&lt;Error&gt;(this); }
    const Error* error() const { return std::get_if&lt;Error&gt;(this); }
    T* value() { return std::get_if&lt;T&gt;(this); }
    const T* value() const { return std::get_if&lt;T&gt;(this); }
    T* operator-&gt;() { return value(); }
    const T* operator-&gt;() const { return value(); }
    operator bool() const { return error() == nullptr; }
};
</code></pre>
<p>We derive from <code>std::variant</code> to take advantage of the ergonomic constructors and assignment operators (meaning we don’t have to implement them for each type and each reference type; doing this correctly is tedious). After this, we add two convenient methods to access the underlying types (along with const overloads). And finally, define <code>operator-&gt;</code> to allow access to the underlying success type value and <code>operator bool</code> to determine whether it holds the success type (along with const overloads).</p>
<p>To demonstrate basic usage we can create a small type hierarchy with success and failure types:</p>
<pre><code class="language-c++">struct SuccessResult {
  int date;
  double time;
};
struct ErrorResult {
  std::string message;
};
using ProcessResult = Result&lt;SuccessResult, ErrorResult&gt;;
</code></pre>
<p>Then we create a function to demonstate it’s usage:</p>
<pre><code>ProcessResult process(std::string_view input) {
  return ErrorResult{ &quot;Not implemented&quot; };
}
</code></pre>
<p>And finally we call the function and write some code that observes the return value:</p>
<pre><code>const auto result = process(&quot;Hello&quot;);
if (!result) {
  std::cout &lt;&lt; &quot;Error: &quot; &lt;&lt; result.error()-&gt;message &lt;&lt; std::endl;
} else {
  std::cout &lt;&lt; &quot;Date: &quot; &lt;&lt; result-&gt;date &lt;&lt; &quot; Time: &quot; &lt;&lt; result-&gt;time &lt;&lt; std::endl;
}
</code></pre>
<p>Overall, this approach is highly ergonomic, concise and clear. The call site is easily understood since <code>operator bool</code> cleanly checks the status, <code>operator -&gt;</code> eliminates unnecessary intermediate variables and method calls. The <code>error()</code> access method is also self-explanatory.</p>
<h2 id="data-or-pointer-to-data">Data Or Pointer to Data</h2>
<p>Another potentially useful class we can build is <code>DataOrPointer</code>. This is a class that either holds a type or a pointer to that type. Again we provide ergonomic access methods and operators:</p>
<pre><code class="language-c++">template&lt;typename T&gt;
class DataOrPointer : public std::variant&lt;T, T*&gt; {
public:
  using std::variant&lt;T, T*&gt;::variant;
  using std::variant&lt;T, T*&gt;::operator=;
  operator const T&amp;() const {
    if (auto value = std::get_if&lt;T&gt;(this); value) {
      return *value;
    }
    return *std::get&lt;T*&gt;(*this);
  }
};
</code></pre>
<p>You can use this if you want to provide a single return type to a function that takes a <code>T</code> as an argument and may or may not return a newly constructed <code>T</code> object after testing some conditions. If constructing <code>T</code> is expensive, this approach can be a clean way to achieve the objective. A real-world example of this could be normalizing two BigFloats to use the same exponent before operating on them. If the target exponent is equal to the current exponent, it would be a waste to build a new copy (In this case we can’t mutate the original numbers).</p>
<pre><code>DataOrPointer&lt;const UnsignedBigFloat&gt; usingExponent(const UnsignedBigFloat&amp; value, int64_t exponent)  {
    if (exponent == value._exponent) {
        return DataOrPointer&lt;const UnsignedBigFloat&gt;(&amp;value);
     }

    auto copy = value;
    copy._mantissa.timesTenToThe(exponent - value._exponent);
    copy._exponent = exponent;
    return std::move(copy);
}
</code></pre>
<h2 id="multi-type-reference">Multi-Type Reference</h2>
<p>I will admit the following example is almost too esoteric to be useful, but I have actually reached for this once before.</p>
<p>The challenge: create a reference that can bind to one of multiple types, performance not being critical and reducing code duplication being the main objective.</p>
<p>Solution:</p>
<pre><code class="language-c++">template&lt;typename... Ts&gt;
class MultiTypeReference : public std::variant&lt;Ts*...&gt; {
public:
  using std::variant&lt;Ts*...&gt;::variant;

  template&lt;typename T&gt;
  auto&amp; operator=(T value) {
    std::visit([&amp;](auto&amp; pointer) { *pointer = value; }, *this);
    return *this;
  }

  template&lt;typename T&gt;
  operator T() {
    return std::visit([&amp;](auto&amp; pointer) { return T(*pointer); }, *this);
  }
</code></pre>
<p>Yes, this works. Yes, it’s weird. No, I don’t encourage you to use it. However, it is an interesting case study and hopefully gets you thinking about how to stretch the use of <code>std::variant</code>. Here’s how one would actually use it:</p>
<pre><code>struct Data {
   bool useFieldA;
   int32_t A;
     int64_t B;
};

MultiTypeReference&lt;int32_t, int64_t&gt; getRelevantField(Data&amp; data) {
    return data.useFieldA ?
          MultiTypeReference&lt;int32_t, int64_t&gt;(data.A) :
          MultiTypeReference&lt;int32_t, int64_t&gt;(data.B);
}

void assignOne(Data&amp; one) {
  getRelevantField(one) = 1;
}
</code></pre>
<p>At the end of the day, yes, this simply hides the conditional access and assignment behind some abstractions. But in use, it behaves exactly how we want it: a reference to one of multiple types.</p>
<h2 id="epilogue">Epilogue</h2>
<p>The reason I included the word ‘Pattern’ in the title is because these ideas can be extended to theoretically endless types and custom access methods.</p>
<p>For examples, you could build a result type with three different possible types and provide access methods for them (removing <code>operator-&gt;</code>). Or perhaps you provide a <code>transform</code> method that takes a functor and changes the value to hold a different type after transforming the current type. Or even just provide custom comparison operators (Which could be necessary for sorting different numeric types; imagine you want to sort a vector of regular int and BigInt references stored within a variant-type).</p>
<p>In review, the key tools to leverage are:</p>
<ol>
<li>Using the constructors and assignment operators provided by <code>std::variant</code>.</li>
<li>Defining named access methods for different types.</li>
<li>Providing an <code>operator-&gt;</code> for a success or special type.</li>
<li>Providing conversion operators for syntax-less unpacking.</li>
</ol>
<p>I hope my discussion of these concepts was useful or at least interesting. Thanks for reading! Subscribe via RSS or <a href="https://www.linkedin.com/in/dgski/">LinkedIn</a>.</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>C++ Iterator-Friendly Branchless Binary Search</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/c-iterator-friendly-branchless-binary-search/"/>
        <id>https://tech.davidgorski.ca/c-iterator-friendly-branchless-binary-search/</id>

        <updated>2023-12-18T09:36:35-05:00</updated>
            <summary>
                <![CDATA[
                    Once you delve into the realm of low-latency C++, you will find&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>Once you delve into the realm of low-latency C++, you will find yourself waking up in the middle of the night, sweating profusely from a nightmare concerning unnecessary branching. And soon after, you begin to over-optimize your code to avoid branches. Even when that part of your code is clearly not the bottleneck.</p>
<p>This is a short post presenting a C++ iterator-friendly implementation of a branchless binary search implementation. It is short and sweet, so I will reveal it before some editorial comments and thanks:</p>
<pre><code>template&lt;typename It&gt;
It lower_bound(It begin, It end, const typename It::value_type&amp; value) {
  auto len = std::distance(begin, end);
  if (len == 0) {
    return end;
  }

  while (len &gt; 1) {
    const auto half = len / 2;
    begin += (*(begin + half - 1) &lt; value) * half;
    len -= half;
  }
  return (*begin &lt; value) ? end : begin;
}
</code></pre>
<p>This implementation uses a <code>begin</code> iterator and <code>len</code> integer to keep track of the search space rather than begin and end pointers/indices. So the first line of the function are simply using the provided iterators to derive the needed variables. We terminate early if the range is empty. All classic binary search branches within the <code>while</code> loop body have been removed:</p>
<ul>
<li>Shrinking the search space length without branches is easy; it will always halve. We can do this safely by subtracting <code>halfLen</code> from the remaining <code>len</code> (This will potentially leave an array of size 1).</li>
<li>Calculating the new search space start is more tricky. It will either be the same <code>begin</code> as now, or be the halfway point between pos and the end of the search space. So we conditionally add <code>halfLen</code> to <code>begin</code>.</li>
<li>To conform to the <code>std::lower_bound</code> interface, we must return the end iterator if the value is not found within the range. To do so, unfortunately we must add a final conditional at the end of the function. Thanks to <a href="https://www.linkedin.com/in/farid-mehrabi/">Farhid Mehrabi</a> for pointing this bug out in my initial implementation (I would potentially return a value that is less than the value, breaking the lower bound contract). Adding a simple unit test would have prevented my from doing so, so this a reminder to myself to do so even for small pieces of code.</li>
</ul>
<p>For the most part, this implementation will have superior performance to <code>std::lower_bound</code>. There is caveat: since the addition to <code>begin</code> will most likely be turned into a <code>CMOV</code> instruction rather than a proper branch, there will be no prediction to preload the next search space mid-points and at larger array sizes, a classic approach will prevail. This can potentially be mitigated on some platforms by adding an explicit <code>prefetch</code> instruction.</p>
<p>Thanks to <a href="https://en.algorithmica.org/hpc/data-structures/binary-search/">this amazing article</a> for outlining this concept using raw arrays.</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>Nanoseconds taken to find 1000 random numbers that are in the range on Macbook Air M1. <a href="https://gist.github.com/dgski/c48142bcb96cc3bf0bf14fe1072e403f">Code</a>.</p>
<pre><code>=============================
array size = 1
branchlessTime = 18000
stdTime        = 18000
=============================
array size = 2
branchlessTime = 18000
stdTime        = 30000
=============================
array size = 4
branchlessTime = 19000
stdTime        = 39000
=============================
array size = 8
branchlessTime = 19000
stdTime        = 48000
=============================
array size = 16
branchlessTime = 20000
stdTime        = 59000
=============================
array size = 32
branchlessTime = 21000
stdTime        = 69000
=============================
array size = 64
branchlessTime = 21000
stdTime        = 78000
=============================
array size = 128
branchlessTime = 22000
stdTime        = 91000
=============================
array size = 256
branchlessTime = 24000
stdTime        = 103000
=============================
array size = 512
branchlessTime = 28000
stdTime        = 113000
=============================
array size = 1024
branchlessTime = 31000
stdTime        = 127000
=============================
array size = 2048
branchlessTime = 35000
stdTime        = 139000
=============================
array size = 4096
branchlessTime = 38000
stdTime        = 146000
=============================
array size = 8192
branchlessTime = 42000
stdTime        = 140000
=============================
array size = 16384
branchlessTime = 40000
stdTime        = 139000
=============================
array size = 32768
branchlessTime = 45000
stdTime        = 149000
=============================
array size = 65536
branchlessTime = 64000
stdTime        = 189000
=============================
array size = 131072
branchlessTime = 79000
stdTime        = 216000
=============================
array size = 262144
branchlessTime = 91000
stdTime        = 233000
=============================
array size = 524288
branchlessTime = 105000
stdTime        = 260000
=============================
array size = 1048576
branchlessTime = 195000
stdTime        = 351000
=============================
array size = 2097152
branchlessTime = 142000
stdTime        = 333000
=============================
array size = 4194304
branchlessTime = 397000
stdTime        = 444000
=============================
array size = 8388608
branchlessTime = 931000
stdTime        = 961000
=============================
array size = 16777216
branchlessTime = 1159000
stdTime        = 996000
=============================
array size = 33554432
branchlessTime = 1327000
stdTime        = 1132000
=============================
array size = 67108864
branchlessTime = 1507000
stdTime        = 1578000
=============================
array size = 134217728
branchlessTime = 1603000
stdTime        = 1710000
=============================
array size = 268435456
branchlessTime = 5921000
stdTime        = 5980000
=============================
array size = 536870912
branchlessTime = 23558000
stdTime        = 15002000
</code></pre>
<p>As you can see the benefits start to deteriorate as the array size grows.</p>

            ]]>
        </content>
    </entry>
    <entry>
        <title>Truncating String White-space At Compile Time in C++</title>
        <author>
            <name>David Gorski</name>
        </author>
        <link href="https://tech.davidgorski.ca/truncating-string-white-space-at-compile-time-in-c/"/>
        <id>https://tech.davidgorski.ca/truncating-string-white-space-at-compile-time-in-c/</id>
            <category term="constexpr"/>
            <category term="Programming"/>
            <category term="C++"/>

        <updated>2023-07-29T14:33:32-04:00</updated>
            <summary>
                <![CDATA[
                    One problem that arises when interleaving SQL queries in C++ code is&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <p>One problem that arises when interleaving SQL queries in C++ code is string literal formatting and spacing. Most queries are much more human-digestible in a multi-line format. C++ treats adjacent string literals as one, so the traditional C++ solution is this:</p>
<pre><code class="language-c++">const char* query =
    &quot; SELECT &quot;
        &quot; u.id, &quot;
        &quot; u.user_name, &quot;
        &quot; u.ref_id, &quot;
        &quot; u.postal_code, &quot;
        &quot; u.email, &quot;
        &quot; o.transaction.id &quot;
    &quot; FROM &quot;
        &quot; users u &quot;
    &quot; JOIN &quot;
        &quot; orders o ON o.user_id = u.id &quot;
    &quot; WHERE &quot;
        &quot; u.id=? AND u.active=? &quot;;
</code></pre>
<p>A programmer has to be very careful with the white-space before and after the SQL tokens as their presence or lack-thereof could be the difference between a good night’s sleep, or a run-time exception triggering a on-call alert late at night. Fun! Overall this code snippet is much more difficult to read due to the syntactical mess of quotation marks around it. To help prevent issues I have even heard this old platitude from a veteran C++ programmer: “A good surgeon washes his hands before and after the surgery”; mandating spaces at the beginning and end of each line. Is a rule of thumb really a best practice? One’s best work is done with as little distraction as possible, and this is something else to worry about.</p>
<p>Modern C++ provides us with one useful addition that can help mitigate this issue: the raw string literal. Leveraging this feature we can write the query naturally; all within one pair of quotations:</p>
<pre><code class="language-c++">const char* query = R&quot;(
    SELECT
        u.id,
        u.user_name,
        u.ref_id,
        u.postal_code,
        u.email,
        o.transaction.id
    FROM
        users u
    JOIN
        orders o ON o.user_id = u.id
    WHERE
        u.id=? AND u.active=?
)&quot;;
</code></pre>
<p>This works rather well. You can define the same exact query, presented very simply and clear. You don’t have to ‘worry’ about the C++ surrounding the SQL. You can just focus on the SQL. The problem is, all those spaces, tabs and newline characters are also in the final query string. The raw string is <strong>298 characters</strong> long versus the original <strong>166 characters</strong>. This means you pass a query string that is almost <strong>double in length</strong> to any query parsing function/module. Now, with the performance of modern hardware this penalty is negligible. However, sometimes, this code is ran in a hot path or you might have a sense of guilt for such an obvious efficiency sacrifice. Let’s make things right in the universe…</p>
<p>The first, conceptually simplest, obvious solution is to run a string-slimming function on all queries when program execution starts. This approach is good enough, but all sub-approaches in this branch have disadvantages:</p>
<ul>
<li>If you want the query to be ready before a certain function is called, you will unfortunately have to move the query variable out of it’s logical code block. This can reduce the clarity of intent and logic locality.</li>
<li>If you want to keep the query nested where it’s used, you will have to devise some kind of static initialization function that runs the first time the function is called. A static flag will have to be checked every time the function is called to determine whether the string has been ‘slimmed’ yet.</li>
<li>You will need to allocate a new destination buffer for the ‘slimmed’ query.</li>
</ul>
<p>As suggested by this article’s title we can actually do this string processing at compile time, with no penalty to our code’s logic or clarity. </p>
<h2 id="contexpr-variables-functions-and-classes">‘contexpr’ Variables, Functions and Classes</h2>
<p>Modern C++ provides the <a href="https://en.cppreference.com/w/cpp/language/constexpr">constexpr</a> functionality to provide compile-time code utilities. There are a lot of resources available online detailing this, so I won’t go into detail. Fundamentally, you can mark your functions, classes and variables with the <strong>constexpr</strong> keyword to signify they could potentially be called or created at compile time. This is not a guarantee, but a contract of sorts. A summary of the rules:</p>
<ul>
<li>A <strong>constexpr variable</strong> must be initialized as a constexpr type with a constepxr function. These must be constructed/called with either literals or other constexpr variables as arguments.</li>
<li>A <strong>constexpr function</strong> can only take and return constexpr types. The key thing to know is that arguments can be accepted as constexpr, but not passed on as constexpr. Once within the function you cannot guarantee the argument values are known at compile time, even though they might be.</li>
<li>A <strong>constexpr type/class</strong> can only have constexpr type members and must at least one constexpr constructor. All scalar types and arrays are considered constexpr.</li>
</ul>
<p>I’m fairly new to <strong>constexpr</strong> myself, so please do your own research, and reach out if I’m incorrect.</p>
<h2 id="implementing-a-compile-time-string">Implementing a Compile Time String</h2>
<p>In order to perform our string parsing/creation at compile time, we need to implement a string class that satisfies the constexpr ‘contract’ for types. We must ensure that all methods we want to call after the constexpr variable construction are marked as ‘const’. The implementation is pretty self explanatory when keeping in mind the restrictions:</p>
<pre><code class="language-c++">namespace compiletime {

template&lt;std::size_t MaxSize = 30&gt;
class string
{
    char m_data[MaxSize] = { 0 };
    std::size_t m_size;
public:
    constexpr string() : m_data({}), m_size(0) {}
    constexpr string(const char* str) : m_data(), m_size(0) {
        for(int i =0; i&lt;MaxSize; ++i) {
            m_data[m_size++] = str[i];
        }
    }
    
    constexpr char const* data() const { return m_data; }
    constexpr operator const char*() const { return data(); } // for convenience
    constexpr void push_back(char c) { m_data[m_size++] = c; }
    constexpr char&amp; operator[](std::size_t i) { return m_data[i]; }
    constexpr char const&amp; operator[](std::size_t i) const { return m_data[i]; }
    constexpr size_t size() const { return m_size; }
    constexpr const char* begin() const { return std::begin(m_data); }
    constexpr const char* end() const { return std::begin(m_data) + m_size; }
};

}
</code></pre>
<h2 id="2-implementing-a-compile-time-parser">2. Implementing a Compile Time Parser</h2>
<p>Now that we have a string class that can used with <strong>constexpr variables</strong>, we can run <strong>constepxr</strong> functions that take this string type as arguments. Remember, in constexpr functions all arguments can be accepted as constexpr, but not passed as constexpr. Additionally, we can only use other constexpr functions and types within the function.</p>
<p>First, let’s implement a simple function which checks if a provided character is a whitespace character:</p>
<pre><code class="language-c++">constexpr bool is_whitespace(char c) {
    return
        (c == &#39; &#39;) ||
        (c == &#39;\t&#39;) ||
        (c == &#39;\n&#39;) ||
        (c == &#39;\v&#39;) ||
        (c == &#39;\f&#39;) ||
        (c == &#39;\r&#39;);
}
</code></pre>
<p>Notice the <em>constexpr</em> keyword as well as the fact that all types used (char) are constexpr-friendly. The actual ‘business logic’ is self-explanatory.</p>
<p>Moving on and focusing on our goal, the function we want to implement takes one <strong>compiletime::string</strong>, iterates over it, and removes consecutive white-space and newlines. This can be done via a simple for-loop, with <strong>push_back</strong> calls appending the preserved characters to a new <strong>compiletime::string</strong> instance. A few predicates allow us to keep track of the parser state. Check it out:</p>
<pre><code class="language-c++">template&lt;std::size_t N&gt;
constexpr auto truncateWhitespace(compiletime::string&lt;N&gt; str) {
    // Need to use non-type template for string max size
    compiletime::string&lt;N&gt; result;
    bool previousIsWhitespace = false; // Keep track if the previous character was whitespace
    for(char c : str) {
        // Skip new lines
        if(c == &#39;\n&#39;) {
            continue;
        } else if(is_whitespace(c)) {
            // If the last character was whitespace, continue interation
            if(previousIsWhitespace) {
                continue;
            }
            // Whitespace: Set flag
            previousIsWhitespace = true;
        } else {
            // Not whitespace: Reset flag
            previousIsWhitespace = false;
        }

        result.push_back(c); // Otherwise; add character to new string
    }
    return result;
}
</code></pre>
<p>And finally, we can provide a simple overload to allow direct string literal use:</p>
<pre><code class="language-c++">template&lt;std::size_t N&gt;
constexpr auto truncateWhitespace(const char (&amp;str)[N])
{
    compiletime::string&lt;N&gt; tmp(str); // build instance
    return truncateWhitespace(tmp); // run function
}
</code></pre>
<h2 id="the-spoils">The Spoils</h2>
<p>Now, we can actually use this construct in our code:</p>
<pre><code class="language-c++">constexpr auto query = R&quot;(
    SELECT
        u.id,
        u.user_name,
        u.ref_id,
        u.postal_code,
        u.email,
        o.transaction.id
    FROM
        users u
    JOIN
        orders o ON o.user_id = u.id
    WHERE
        u.id=? AND u.active=?
)&quot;;

constexpr auto trucatedQuery = truncateWhitespace(query);
std::cout &lt;&lt; trucatedQuery;
// output:
// &quot; SELECT u.id, u.user_name, u.ref_id, u.postal_code, u.email, o.transaction.id FROM users u JOIN orders o ON o.user_id = u.id WHERE u.id=? AND u.active=? &quot;
</code></pre>
<p>The new query length is <strong>154 characters</strong>! Which beats obviously beats the original raw literal (294) and the old “spaces before and after” mantra with traditional one-line string literals (162). And we can validate that this is actually happening at compile time with a <a href="https://www.google.com/search?channel=fs&amp;client=ubuntu&amp;q=static+assert">static_assert</a>. If you are using a modern IDE/Editor with C++ integration the following statement will actually be highlighted as a failed assertion before running compilation:</p>
<pre><code>static_assert(trucatedQuery.size() == 154);
</code></pre>
<p>We have succeeded and in the process gained advantages over both previous approaches:</p>
<ul>
<li><strong>Clearer and less cluttered code site</strong>: Freedom to use raw string literals to format queries with as much indentation as you’d like.</li>
<li><strong>Neater, Compact Logging</strong>: New lines scattered in log files make them harder to parse and understand.</li>
<li><strong>Less Network Bandwidth Usage</strong>: If you are using a database that directly accepts the query string, you will be sending half the bytes over the network.</li>
<li><strong>Faster Performance</strong>: If you do any local string validation/processing/formatting, you will gain some performance.</li>
</ul>
<p>For the sake of it, I did a simple benchmark comparing the generated <strong>compile_time::string</strong> buffer to a <strong>const char</strong>* with the exact same literal. On my system, iterating over the provided string using a simple pointer and while loop resulted in consistently faster performance averaged over many iterations. I would be greatly concerned if it didn’t, as there is now half as many characters as before! A simple O(N) iteration operation will obviously be faster on less elements.</p>
<p>I can see this technique providing excellent payoff for queries with many joins and deeper indentation (maybe in a lambda inside a class member function or something), sent over the network and parsed on a regular, frequent basis. It’s also just feels great, knowing you’ve reached the holy grail of performance; aka compile time operations.</p>
<h2 id="limitations">Limitations</h2>
<p>There is one obvious limitation of this approach. Even though the ‘string’ value is shortened, we pack a buffer of the original size into the executable, padded with zeroes at the end of the string. This is because the final string size is needed as a compile time template value to construct the string class. Unfortunately, once we’re inside the <strong>truncateWhitespace</strong> function, we can’t retrieve the length to use with the string type. We also can’t make any assumptions about what the final size will be as there may not be any superfluous white-space (A simple N / 2 estimate could result in compilation failures in some cases). You could solve this this by implementing a constexpr function to calculate the final length before doing the actual truncation. Something like this:</p>
<pre><code class="language-c++">constexpr const char originalQuery[] = R&quot;(...)&quot;;
constexpr std::size_t slimmedSize = calculateTruncatedWhitespaceSize(originalQuery);
constexpr auto trucatedQuery = truncateWhitespace&lt;slimmedSize&gt;(originalQuery);
</code></pre>
<p>I cannot currently think of a way to do this in a way does not leverage a macro, as we can’t create function to wrap this functionality, since the arguments would not be considered constexpr inside and therefore could not be used calculate a constexpr length to be provided as a non-type template parameter to the string class. Please let me know if you can suggest a technique or trick!</p>
<p>Another limitation is using string literals within the SQL itself. Luckily, this can be mitigated by enhancing the parser to keep track if the current point of iteration is within a string literal or not.</p>
<h2 id="in-closing">In Closing</h2>
<p>Though the performance gains are negligible in most cases, this approach provides real benefits in clarity, logging and efficiency. You can leverage this same technique to do any kind of string pre-processing at compile time as needed.</p>
<p><a href="https://gist.github.com/dgski/810ede7c4a80917c0adc99c6852fee9a">Link to Complete Code</a></p>

            ]]>
        </content>
    </entry>
</feed>
