<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Introduction To Low Latency Programming: Understand Storage - Tech @ DG</title><meta name="description" content="This post originally appears as a chapter in my new book: ‘Introduction&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://tech.davidgorski.ca/introduction-to-low-latency-programming-understand-storage/"><link rel="alternate" type="application/atom+xml" href="https://tech.davidgorski.ca/feed.xml"><link rel="alternate" type="application/json" href="https://tech.davidgorski.ca/feed.json"><meta property="og:title" content="Introduction To Low Latency Programming: Understand Storage"><meta property="og:site_name" content="Tech @ DG"><meta property="og:description" content="This post originally appears as a chapter in my new book: ‘Introduction&hellip;"><meta property="og:url" content="https://tech.davidgorski.ca/introduction-to-low-latency-programming-understand-storage/"><meta property="og:type" content="article"><link rel="preload" href="https://tech.davidgorski.ca/assets/dynamic/fonts/publicsans/publicsans.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="https://tech.davidgorski.ca/assets/css/style.css?v=1501a03667953b81505b3bfe44bcde3f"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://tech.davidgorski.ca/introduction-to-low-latency-programming-understand-storage/"},"headline":"Introduction To Low Latency Programming: Understand Storage","datePublished":"2024-03-04T12:01","dateModified":"2024-03-04T12:01","description":"This post originally appears as a chapter in my new book: ‘Introduction&hellip;","author":{"@type":"Person","name":"David Gorski","url":"https://tech.davidgorski.ca/authors/david-gorski/"},"publisher":{"@type":"Organization","name":"David Gorski"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript><script async defer="defer" data-website-id="f50301b5-5202-4a86-9509-d0a3ed6ed290" src="https://analytics.xantasoft.com/umami.js"></script></head><body><div class="content"><div class="left-bar"><div class="left-bar__inner"><header class="header"><a class="logo" href="https://tech.davidgorski.ca/">Tech @ DG</a></header></div></div><main class="main"><article class="post"><div class="post__meta post__meta--attop"><div class="post__meta--attop__inner"><div class="post__maintag"><svg width="20" height="20" aria-hidden="true" focusable="false"><use xlink:href="https://tech.davidgorski.ca/assets/svg/svg-map.svg#tag"/></svg> Published in <a href="https://tech.davidgorski.ca/tags/c/" class="metadata__maintag">C++</a></div></div></div><div class="main__inner"><div class="post__meta"><div class="post__author"><div><a href="https://tech.davidgorski.ca/authors/david-gorski/" class="post__author__name">David Gorski</a></div></div><div class="post__date"><time datetime="2024-03-04T12:01">Mar 4, 2024</time></div></div><header class="post__header"><h1 class="post__title">Introduction To Low Latency Programming: Understand Storage</h1></header><div class="post__entry"><p><em>This post originally appears as a chapter in my new book: ‘Introduction To Low Latency Programming’, a short and approachable entry into the subject. <a href="https://a.co/d/0U6KOfb">Available now for purchase on Amazon</a>.</em></p><p>So far we have focused on the computation aspect of low latency programming. However, optimized data access plays a huge role in this domain. No process is computation only, it needs data to compute on. This chapter will discuss the key ideas to keep in mind for the optimization of reading and writing from storage.</p><h2 id="be-aware-storage-performance-and-access-costs">Be Aware Storage Performance And Access Costs</h2><p>I am using the word ‘storage’ quite broadly in this chapter; referring to persistent storage, temporary storage, local storage and network storage. From our perspective, we mainly care about ‘time to access’, rather than durability, geographic location or any other ‘implementation’ details.</p><p>It is very important to know the rough time it takes to access different storage sinks. In general, this time increases exponentially as the physical distance from the actual CPU core increases. The following list is ordered from closest to furthest for a theoretical CPU core including the rough ‘time to access’ expressed as a multiplier:</p><ol><li>Registers: 1x</li><li>L1 cache: 5x</li><li>L2 cache: 10x</li><li>L3 cache: 50x</li><li>Same-NUMA memory: 100x</li><li>Different-NUMA memory: 300x</li><li>Persistent Storage (SSD/HDD/Tape): 10000x minimum</li><li>Local Data Center: 200000x</li><li>Distant Data Center: 100000000x</li><li>The Moon: 1000000000000x</li></ol><p>Our closest example storage type is the processor-local set of registers. These are the fastest accessible locations to the processor and are used as inputs and outputs for various computations. Most computers typically operate by loading data from the main memory into these registers, performing some instructions which save results to a register, and finally writing the values from registers into the main memory. Our code is turned into instructions which work with registers. The various levels of cache are optimizations for accessing the main memory.</p><p>The furthest example storage type is a theoretical network drive that is within a computer on the moon. If a program wants to read or write to it, it will have to issue the command over a network spanning 384,400 kilometers. It is obvious that until we develop spacetime wormholes, greater physical distance translates into higher communication latency.</p><p>Keep this list in mind when designing your programs. For low-latency applications, you will ideally never go beyond ‘same-NUMA memory’ after initial start-up is complete. <em>NUMA (Non-Uniform Memory Architecture)</em> is a multiprocessing architecture feature that assigns memory to specific CPU cores in an effort improve overall memory performance. The key takeaway: <em>try to keep your data as close the CPU as possible</em>. Of course, if our program has data persistence requirements, you will factor that in to your design.</p><h2 id="caching">Caching</h2><p>With most modern architectures, continuous memory is loaded together as a group. When a memory address is requested to be read, surrounding data will be pulled into the various cache layers as well. This group of data is called a ‘cache line’. You can see the size of the cache line on your platform with the <strong>std::hardware_destructive_interference_size</strong> value in C++.</p><p>Keeping cache mechanics in mind is important as you design your algorithms and data structures. Factoring them into your program implementation can greatly increase performance and therefore reduce latency. In general, to keep a portion of code running as fast as possible, have it access as small a range of continuous memory as possible.</p><p>Put data that will be accessed as ‘a unit’ as close as possible; ideally within one cache-line for maximum performance. There are still huge benefits for using continuous memory beyond that. For example, adjacent cache lines are often prefetched. Generally use data structures that are as flat as possible (in contiguous memory) like static or dynamic arrays. They should be the default data structure of choice. Additionally, design your algorithms to fully process a block of contiguous memory before moving on (avoid going back). Sometimes this means re-arranging the order of some loops, such as with this simple matrix multiplication function:</p><p>Before:</p><pre><code class="language-c++">using Vector = std::vector&lt;float&gt;;
using TwoDimMatrix = std::vector&lt;Vector&gt;;

TwoDimMatrix matMul(const TwoDimMatrix&amp; m1, const TwoDimMatrix&amp; m2)
{
  const auto sharedDim = m1.front().size();
  TwoDimMatrix result(m1.size(), Vector(m2.front().size()));
  // Iterate over rows of m1
  for (size_t i = 0; i &lt; m1.size(); ++i) {
    // Iterate over rows of m2
    for (size_t j = 0; j &lt; m2.size(); ++j) {
      // Iterate over cells of m1 row and m2 column
      for (size_t k = 0; k &lt; sharedDim; ++k) {
        result[i][j] += m1[i][k] * m2[k][j];
      }
    }
  }
  return result;
}
</code></pre><p>After:</p><pre><code class="language-c++">TwoDimMatrix matMul(const TwoDimMatrix&amp; m1, const TwoDimMatrix&amp; m2)
{
  const auto sharedDim = m1.front().size();
  TwoDimMatrix result(m1.size(), Vector(m2.front().size()));
  // Iterate over rows of m1
  for (size_t i = 0; i &lt; m1.size(); ++i) {
    // Iterate over cells of the m1 row and rows of m2
    for (size_t k = 0; k &lt; sharedDim; ++k) {
      // Iterate over cells of the m2 row
      for (size_t j = 0; j &lt; m2.size(); ++j) {
        result[i][j] += m1[i][k] * m2[k][j];
      }
    }
  }
  return result;
}
</code></pre><p>All we really did was switch the k and j loop nesting order. This change can yield up to a 10x speed up as iterating over a column is much more expensive than iterating through a row. Why? The row is continuous in memory, whereas the column is not. Every time we access the next value in the column we are actually accessing a relatively ‘distant’ part of memory.</p><p>Another thing to keep in mind is ‘false sharing’. If you are using atomic instructions on two memory locations that are within a single cache line, you could inadvertently slow down your program as the CPU will send an instruction to lock the cache line from other cores. This is unavoidable and desired if the two cores are actually accessing the same memory location, but if it just happens to be nearby and not the <em>same exact</em> memory location, we are paying a price for nothing. To mitigate this, you want to make sure the two memory addresses are on separate cache lines using the C++ <code>alignas</code> keyword:</p><pre><code class="language-c++">struct DoNotShare {
  alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; one;
  alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; two;
};
</code></pre><p>This will prevent any false sharing from occurring by aligning the variables to offsets defined by the integer value returned by <strong>std::hardware_destructive_interference_size</strong>. Keep this in mind when using atomic instructions on nearby memory addresses.</p><h2 id="chapter-summary">Chapter Summary</h2><ul><li>Know the rough time taken to access different storage sinks. From same-NUMA memory cache line all the way to a computer sending RF signals from the moon.</li><li>Understand the cache characteristics of your hardware.</li><li>Keep in mind that that measurement is key; educated assumptions can be useful early on in the design process, but to derive conclusions you must measure performance yourself.</li></ul></div><footer class="post__footer"><div class="post__last-updated">This article was updated on <time datetime="2024-03-04T12:01">Mar 4, 2024</time></div><div class="post__share"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ftech.davidgorski.ca%2Fintroduction-to-low-latency-programming-understand-storage%2F" class="js-share facebook tltp tltp--top" aria-label="Facebook" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://tech.davidgorski.ca/assets/svg/svg-map.svg#facebook"/></svg> <span>Facebook</span> </a><a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Ftech.davidgorski.ca%2Fintroduction-to-low-latency-programming-understand-storage%2F&amp;via=Tech%20%40%20DG&amp;text=Introduction%20To%20Low%20Latency%20Programming%3A%20Understand%20Storage" class="js-share twitter tltp tltp--top" aria-label="Twitter" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://tech.davidgorski.ca/assets/svg/svg-map.svg#twitter"/></svg> <span>Twitter</span> </a><a href="https://pinterest.com/pin/create/button/?url=https%3A%2F%2Ftech.davidgorski.ca%2Fintroduction-to-low-latency-programming-understand-storage%2F&amp;media=undefined&amp;description=Introduction%20To%20Low%20Latency%20Programming%3A%20Understand%20Storage" class="js-share pinterest tltp tltp--top" aria-label="Pinterest" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://tech.davidgorski.ca/assets/svg/svg-map.svg#pinterest"/></svg> <span>Pinterest</span> </a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Ftech.davidgorski.ca%2Fintroduction-to-low-latency-programming-understand-storage%2F" class="js-share linkedin tltp tltp--top" aria-label="Share with LinkedIn" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://tech.davidgorski.ca/assets/svg/svg-map.svg#linkedin"/></svg> <span>LinkedIn</span></a></div></footer></div></article><div class="post__section post__related"><div class="main__inner"><h3 class="post__section__title">Related post</h3><div class="post__related__wrap"><article class="c-card"><div class="c-card__meta"><div class="c-card__author"><a href="https://tech.davidgorski.ca/authors/david-gorski/">David Gorski</a></div><time datetime="2024-02-26T15:42">Feb 26, 2024 </time><a href="https://tech.davidgorski.ca/tags/c/" class="c-card-tag">C++</a></div><header class="c-card__header"><h2 class="c-card__title"><a href="https://tech.davidgorski.ca/introduction-to-low-latency-programming-minimize-branching-and-jumping/">Introduction To Low Latency Programming: Minimize Branching And Jumping</a></h2><p>This post originally appears as a chapter in my new book: ‘Introduction&hellip;</p></header></article></div></div></div></main><div class="right-bar"><div class="right-bar__inner"><div class="sidebar"><section class="box promo"><h3 class="box__title">New Book Available Now!</h3><img src="/media/files/ilp.png"><p>Introduction To Low Latency Programming: Learn The Fundamental Ideas Behind High-Performance C++ Code</p><br><a href="https://a.co/d/0U6KOfb"><button>Buy On Amazon</button></a><br><br><br><h3 class="box__title">Newsletter Subscription Options</h3><ul><li><a href="https://tech.davidgorski.ca/feed.xml">RSS</a></li><li><a href="https://techatdg.substack.com">Email Via Substack</a></li></ul></section><div class="box follow"><a href="https://twitter.com/thedavidgorski" class="tltp tltp--top" aria-label="Twitter"><svg><use xlink:href="https://tech.davidgorski.ca/assets/svg/svg-map.svg#twitter"/></svg> <span>Twitter</span> </a><a href="https://www.linkedin.com/in/dgski/" class="tltp tltp--top" aria-label="LinkedIn"><svg><use xlink:href="https://tech.davidgorski.ca/assets/svg/svg-map.svg#linkedin"/></svg> <span>LinkedIn</span></a></div><div class="box copyright">© David Gorski, 2024</div></div></div></div></div><script defer="defer" src="https://tech.davidgorski.ca/assets/js/scripts.min.js?v=12d8fcd46db8fdc7af6797ec26849875"></script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script><script defer="defer" src="https://tech.davidgorski.ca/assets/js/quicklink.umd.js?v=a52ee49fe4afff274f8c30fe880ddc13"></script><script>window.addEventListener('load', () =>{
      quicklink.listen();
      });</script></body></html>